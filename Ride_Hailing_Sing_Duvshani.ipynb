{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YovliDuvshani/RideHailling/blob/main/Ride_Hailing_Sing_Duvshani.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "\n",
        "!pip install pyhailing #Restart runtime\n",
        "!pip install --upgrade Pillow #Restart runtime"
      ],
      "metadata": {
        "id": "ldgLzwPpt20I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4d8fe6-7c62-4346-ce48-1937ef51f4a9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyhailing in /usr/local/lib/python3.7/dist-packages (0.0.9)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from pyhailing) (0.17.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pyhailing) (9.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyhailing) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyhailing) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pyhailing) (3.2.2)\n",
            "Requirement already satisfied: numpy<1.22,>=1.19 in /usr/local/lib/python3.7/dist-packages (from pyhailing) (1.21.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->pyhailing) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->pyhailing) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->pyhailing) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhailing) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhailing) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhailing) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyhailing) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->pyhailing) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyhailing) (2018.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (9.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ethDzezC7Vyb"
      },
      "outputs": [],
      "source": [
        "# Import \n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy\n",
        "import numpy as np #To improve\n",
        "import pyhailing\n",
        "from pyhailing import RidehailEnv\n",
        "import tqdm\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import time as t\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import time as __time__\n",
        "from scipy import stats\n",
        "\n",
        "random.seed(2)\n",
        "torch.manual_seed(2)\n",
        "numpy.random.seed(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Documentation \n",
        "**What's the general idea of this code ?**\n",
        "* Env_config : sets the simulation environment in the appropriate configuration\n",
        "\n",
        "* Distance functions : contains all the functions that aims at estimating riding distances or riding durations in the environment, whether these functions are based on mean estimations or probalistic estimations\n",
        "\n",
        "* Environment : contains functions that aims at defining some basic notations in order to simplify the environment representation, also contains the request distribution per 15minutes for the small configuration (for other configurations the distribution just gets multiplied by a coefficient)\n",
        "\n",
        "* Heuristic functions : contains 2 different repositioning heuristics that we tested, the basic one where we relocate the jobless cars; and a more complex one which is more detailled in the corresponding section\n",
        "\n",
        "* Q-learning algorithm : we used two different DeepQ Learning models; one that receives global inputs and cars inputs, then returns Q-values associated to the car to assign to the request or to the no assignment option, and one that only receives global inputs and decides whether or not the request should be assigned, then the car the most likely to arrive before 5 minutes takes the request (if decided that the request should be assigned).\n",
        "An other one is not yet functional and use the advantage concept.\n",
        "\n",
        "* Main : training loops and results are available in this section\n",
        "There are 2 different mains one for the general use and an other specific main for what we call the 'DistilQ Algorithm'. \n",
        "\n",
        "***Every relevant section is also documented in this notebook***"
      ],
      "metadata": {
        "id": "3gF0eqcUoX-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Env_Config"
      ],
      "metadata": {
        "id": "bBlNAPn87XKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the environnment\n",
        "\n",
        "size = 1 #1:small, 2:medium, 3:large\n",
        "if size==1:\n",
        "  env_config = RidehailEnv.DIMACS_CONFIGS.SUI\n",
        "  env_config['nickname'] = 'mynickname'\n",
        "elif size==2:\n",
        "  env_config = RidehailEnv.DIMACS_CONFIGS.MUI\n",
        "  env_config['nickname'] = 'mynickname'\n",
        "else:\n",
        "  env_config = RidehailEnv.DIMACS_CONFIGS.LUI\n",
        "  env_config['nickname'] = 'mynickname'\n",
        "env = RidehailEnv(**env_config)\n",
        "env.num_vehicles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLAa50OI6ps_",
        "outputId": "57f51f6f-76ba-44f6-8dec-01282faa41ed"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te9ytL0gJcIQ"
      },
      "source": [
        "## Distance functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "k9WjgWFV7alG"
      },
      "outputs": [],
      "source": [
        "speeds_data = env.speeds_data.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "c4pmwIAS7dN2"
      },
      "outputs": [],
      "source": [
        "def dist_manhattan(list_coord_depart, list_coord_arrivee):\n",
        "  \"\"\" \n",
        "  Returns the manhattan distance between 2 points of the space\n",
        "  ([x_depart,y_depart],[x_arrivee,y_arrivee]) \n",
        "  \"\"\"\n",
        "  \n",
        "  depart = numpy.array(list_coord_depart)\n",
        "  arrivee = numpy.array(list_coord_arrivee)\n",
        "  return abs(depart[0]-arrivee[0]) + abs(arrivee[1] - depart[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "P6ZuLZir7fAt"
      },
      "outputs": [],
      "source": [
        "def vitesse_normalisee(vitesse_moyenne, sigma):\n",
        "  \"\"\" Returns a random sample of the normal law N(mean_speed,(std_speed)**2) \"\"\"\n",
        "  loi_normale = numpy.random.randn(1000)\n",
        "  loi_normale = [loi_normale[i]*sigma + vitesse_moyenne for i in range(len(loi_normale))]\n",
        "  limite = numpy.quantile(loi_normale, .10)\n",
        "  normale_tronquee = []\n",
        "  for i in range(len(loi_normale)):\n",
        "    loi_normale[i] = numpy.array(loi_normale[i])[0]\n",
        "    if loi_normale[i] >= limite:\n",
        "      normale_tronquee.append(loi_normale[i])\n",
        "  return random.choices(normale_tronquee)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "U0cMbRNH7hIv"
      },
      "outputs": [],
      "source": [
        "def duree_deplacement(list_coord_depart, list_coord_arrivee, time_):\n",
        "  \"\"\" Returns the duration of a trip with the beggining and arrival coordinates and the time:\n",
        "  ([x_depart,y_depart],[x_arrivee,y_arrivee],time(from 0 to 86400)) \"\"\"\n",
        "\n",
        "  distance = dist_manhattan(list_coord_depart, list_coord_arrivee)\n",
        "  depart = numpy.array(list_coord_depart)\n",
        "  arrivee = numpy.array(list_coord_arrivee)\n",
        "  zone_depart = env.xy_to_zone(depart)\n",
        "  zone_arrivee = env.xy_to_zone(arrivee)\n",
        "  tranche_horaire = int(time_/15/60)*15\n",
        "  vitesse_moyenne = speeds_data[(speeds_data['puzone']==zone_depart) & (speeds_data['dozone']==zone_arrivee) & (speeds_data['min']==tranche_horaire)]['speed_mean']\n",
        "  sigma = speeds_data[(speeds_data['puzone']==zone_depart) & (speeds_data['dozone']==zone_arrivee) & (speeds_data['min']==tranche_horaire)]['speed_stddev']\n",
        "  #vitesse_associee = vitesse_normalisee(vitesse_moyenne, sigma) #WAY TO MUCH TIME TO PROCESS\n",
        "  temps = distance/vitesse_moyenne\n",
        "  return numpy.array(temps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fYS9VNmLQ3UE"
      },
      "outputs": [],
      "source": [
        "def distance_to_request(car_coord, req_coord, car_job, time, first_job_coord, second_job_coord, third_job_coord): #A revoir + Prendre en compte les plages horaires. #Vérifier sur exemple.\n",
        "  \"\"\"\n",
        "  Return the time to a request taking into account the jobs of the cars. #Distance -> Time\n",
        "  \"\"\"\n",
        "\n",
        "  car_job = str(car_job[0]) + str(car_job[1]) + str(car_job[2])\n",
        "  if car_job in ['044','444','104']: \n",
        "    return duree_deplacement(car_coord,req_coord,time)\n",
        "  if car_job in ['344']:\n",
        "    duree = duree_deplacement(car_coord,first_job_coord[1],time)\n",
        "    duree += duree_deplacement(first_job_coord[1],req_coord,(time+duree)%86400)\n",
        "    return duree\n",
        "  if car_job == '234':\n",
        "    duree = duree_deplacement(car_coord,first_job_coord[1],time) \n",
        "    duree += duree_deplacement(first_job_coord[1],second_job_coord[0],(time+duree)%86400)\n",
        "    duree += duree_deplacement(second_job_coord[0],second_job_coord[1],(time+duree)%86400)\n",
        "    duree += duree_deplacement(second_job_coord[1],req_coord,(time+duree)%86400)\n",
        "    return duree\n",
        "  duree = duree_deplacement(car_coord,first_job_coord[1],time) \n",
        "  duree += duree_deplacement(first_job_coord[1],second_job_coord[0],(time+duree)%86400)\n",
        "  duree += duree_deplacement(second_job_coord[0],second_job_coord[1],(time+duree)%86400)\n",
        "  duree += duree_deplacement(second_job_coord[1],third_job_coord[0],(time+duree)%86400)\n",
        "  duree += duree_deplacement(third_job_coord[0],third_job_coord[1],(time+duree)%86400)\n",
        "  duree += duree_deplacement(third_job_coord[1],req_coord,(time+duree)%86400)\n",
        "  return duree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def true_distance_to_request(car_coord, req_coord, car_job, first_job_coord, second_job_coord, third_job_coord):\n",
        "  car_job = str(car_job[0]) + str(car_job[1]) + str(car_job[2])\n",
        "  if car_job in ['044','444','104']: \n",
        "    return dist_manhattan(car_coord,req_coord)\n",
        "  if car_job in ['344']:\n",
        "    dist = dist_manhattan(car_coord,first_job_coord[1])\n",
        "    dist += dist_manhattan(first_job_coord[1],req_coord)\n",
        "    return dist\n",
        "  if car_job == '234':\n",
        "    dist = dist_manhattan(car_coord,first_job_coord[1]) \n",
        "    dist += dist_manhattan(first_job_coord[1],second_job_coord[0])\n",
        "    dist += dist_manhattan(second_job_coord[0],second_job_coord[1])\n",
        "    dist += dist_manhattan(second_job_coord[1],req_coord)\n",
        "    return dist\n",
        "  dist = dist_manhattan(car_coord,first_job_coord[1]) \n",
        "  dist += dist_manhattan(first_job_coord[1],second_job_coord[0])\n",
        "  dist += dist_manhattan(second_job_coord[0],second_job_coord[1])\n",
        "  dist += dist_manhattan(second_job_coord[1],third_job_coord[0])\n",
        "  dist += dist_manhattan(third_job_coord[0],third_job_coord[1])\n",
        "  dist += dist_manhattan(third_job_coord[1],req_coord)\n",
        "  return dist"
      ],
      "metadata": {
        "id": "i9ZVRuPBIrGT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilities from distance"
      ],
      "metadata": {
        "id": "n6BjfZ7LxcBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Idea Behind 'Probabilities from distance' Section**\n",
        "\n",
        "The speed we have in our environment is random, it follows a normal law with parameters we know. The problem is, that a request appears for only 5 mins, and so we can never be sure, that the speed sampled of the environnment will allow us to do the request. But we can calculate a probability!\n",
        "\n",
        "When the car is available right away, it's not a problem, we calculate the $1 - cdf(minimum\\ speed\\ to\\ atteign\\ the\\ request)$.\n",
        "\n",
        "That gives us the probability. The difficulty appears when there are multiple trip! For a car in '344',for example ,both portions have speeds which follow different laws. A good way to know would be to do a certain amount of **sample from theses laws and see how many time the car was able to atteign the request**, but the computation time for that is too high and not worth it. So we've taken a different approach:\n",
        "\n",
        "Let's write the problem : We have $X_{1}$ ~ $N_{1}(m_{1},σ_{1})$ and $X_{2}$ ~ $N_{2}(m_{2},σ_{2})$ with $X_{1}$ et $X_{2}$ respectively the speed on the first sub-trip and on the second.\n",
        "\n",
        "We want that : $300 (s) > \\frac{d_{1}}{X_{1}} + \\frac{d_{2}}{X_{2}}$.\n",
        "$d_{1}$ and $d_{2}$ are the distance of the first and second job.\n",
        "The problem is as follow : the sum of 2 inverse normal laws doesn't follow a specific law.\n",
        "\n",
        "So first we consider that every subtrip must be done in less than $d_{α}$/($d_{α}$ + $d_{β}$)$*300(s)$.\n",
        "Which is a lowers significantly the probability that we consider that the job can be done in less than 5 minutes. We'll call $d_{α}$/($d_{α}$ + $d_{β}$) : $p_{α}$. We now have 2 equations:\n",
        "\n",
        " $300(s)*p_{1} > \\frac{d_{1}}{X_{1}}$.\n",
        "\n",
        " $300(s)*p_{2} > \\frac{d_{2}}{X_{2}}$.\n",
        "\n",
        " By simplification, it implied that: $p_{1}X_{1} + p_{2}X_{2} > \\frac{d_{tot}}{300(s)}$, the math underneath isn't going to be explicit but we must verify that when the first condition $300 (s) > \\frac{d_{1}}{X_{1}} + \\frac{d_{2}}{X_{2}}$ is true then $p_{1}X_{1} + p_{2}X_{2} > \\frac{d_{tot}}{300(s)}$ is also true.\n",
        "\n",
        " And $p_{1}X_{1} + p_{2}X_{2}$ ~ $N_{1}(p_{1}m_{1}+p_{2}m_{2},\\sqrt{(p_{1}σ_{1})^2+(p_{2}σ_{2})^2})$.\n",
        "\n",
        " We can now calculate the cdf of this law and get an estimation of the probability (always lower than the truth). A correction could be to multiply the probability by a certain number greater than 1, in order to correct for the bias.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6u5HIlUtNR9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def proba_distance_moins_tmin(list_coord_depart, list_coord_arrivee, time,request_time,eps=1):\n",
        "  \"\"\" Les coordonnées sont en km, et de la forme [x,y].\n",
        "  Prend en entrée les coordonnées de départ, les coordonnées d'arrivée, ainsi que le temps en secondes.\n",
        "  Retourne la probabilité d'effectuer la distance en moins de 5 minutes, la distance, la vitesse moyenne du trajet et \n",
        "  l'écart-type de la vitesse du trajet \"\"\"\n",
        "\n",
        "  distance = dist_manhattan(list_coord_depart, list_coord_arrivee)\n",
        "  depart = numpy.array(list_coord_depart)\n",
        "  arrivee = numpy.array(list_coord_arrivee)\n",
        "  zone_depart = env.xy_to_zone(depart)\n",
        "  zone_arrivee = env.xy_to_zone(arrivee)\n",
        "  tranche_horaire = int(time/15/60)*15\n",
        "  vitesse_moyenne = np.array(speeds_data[(speeds_data['puzone']==zone_depart) & (speeds_data['dozone']==zone_arrivee) & (speeds_data['min']==tranche_horaire)]['speed_mean'])[0]\n",
        "  sigma = np.array(speeds_data[(speeds_data['puzone']==zone_depart) & (speeds_data['dozone']==zone_arrivee) & (speeds_data['min']==tranche_horaire)]['speed_stddev'])[0]\n",
        "# on veut que la vitesse soit supérieure au seuil qui permet de voyager en moins de 5 minutes\n",
        "  vitesse_limite = distance/(300-(time-request_time-eps)) #inégalité pour que distance/vitesse < time-request_time\n",
        "  proba_inf = stats.norm.cdf(vitesse_limite, loc = vitesse_moyenne, scale = sigma)\n",
        "  proba_sup = 1 - proba_inf\n",
        "  return proba_sup, distance, vitesse_moyenne, sigma"
      ],
      "metadata": {
        "id": "vAMmFIn4xepV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proba_request_moins_tmin(car_coord, req_coord, car_job, time, request_time, first_job_coord,eps=1):\n",
        "\n",
        "  # j'ai retiré les cas des voitures en 234 et 323 car elles ne peuvent pas prendre la requête\n",
        "\n",
        "  #intuition d'une distribution proportionnelle afin d'aggréger les probabilités\n",
        "  #on approxime les durées de trajet pour gérer les eventuels changements de tranche horaire, à l'aide des vitesses moyennes\n",
        "  car_job = str(car_job[0]) + str(car_job[1]) + str(car_job[2])\n",
        "  if car_job in ['044','444','104']: \n",
        "    return proba_distance_moins_tmin(car_coord,req_coord,time,request_time)[0]\n",
        "\n",
        "  elif car_job in ['344']:\n",
        "    _, distance1, moyenne1, sigma1 = proba_distance_moins_tmin(car_coord,first_job_coord[1],time,request_time)\n",
        "    duree = dist_manhattan(car_coord,first_job_coord[1])/moyenne1 #pour gérer les changements de tranche horaire\n",
        "\n",
        "    _, distance2, moyenne2, sigma2 = proba_distance_moins_tmin(first_job_coord[1],req_coord,time,request_time) #Should be time+duree but easier.\n",
        "\n",
        "    #approximation sur la durée proportionnelle à la distance\n",
        "    poids1 = distance1/(distance1+distance2)\n",
        "    poids2 = distance2/(distance1+distance2)\n",
        "\n",
        "    # on veut que la vitesse soit supérieure au seuil qui permet de voyager en moins de 300-time-request_time minutes\n",
        "    vitesse_limite1 = distance1/((300-time+request_time+eps)) #on veut que le segment soit parcouru en moins de 5minutes*proportion_du_segment\n",
        "    vitesse_limite2 = distance2/((300-time+request_time+eps)) \n",
        "\n",
        "    vitesse_limite = vitesse_limite1 + vitesse_limite2 \n",
        "    moyenne = poids1*moyenne1 + poids2*moyenne2 \n",
        "    sigma = numpy.sqrt((sigma1*poids1)**2 + (sigma2*poids2)**2)\n",
        "    return 1 - stats.norm.cdf(vitesse_limite, loc = moyenne, scale = sigma)\n",
        "  return 0"
      ],
      "metadata": {
        "id": "ojJ6mms1xx8D"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRulRdiMCUmf"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "niKL902eCUmh"
      },
      "outputs": [],
      "source": [
        "def triplets_jobs(state): \n",
        "  \"\"\" \n",
        "  Take a state and returns the job of every car in a dic\n",
        "  return : dic with car stored \n",
        "  \"\"\"\n",
        "  jobs = state['v_jobs']\n",
        "  dic = {'044':[],'104':[],'234':[],'323':[],'344':[],'444':[]}\n",
        "  for i in range(len(jobs)):\n",
        "    triplet = str(jobs[i][0]) + str(jobs[i][1]) + str(jobs[i][2])\n",
        "    dic[triplet] += [i]\n",
        "  return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "a-6f5JUgCUmi"
      },
      "outputs": [],
      "source": [
        "# Coordonate normalization\n",
        "\n",
        "estimated_mean_x = 587.89\n",
        "estimated_sigma_x = 0.94\n",
        "estimated_mean_y = 4512.1\n",
        "estimated_sigma_y = 3.21\n",
        "def normalize_x(x):\n",
        "  \"\"\"\n",
        "  Normalize the x coord\n",
        "  \"\"\"\n",
        "  return (x-estimated_mean_x)/estimated_sigma_x\n",
        "def normalize_y(y):\n",
        "  \"\"\"\n",
        "  Normalize the y coord\n",
        "  \"\"\"\n",
        "  return (y-estimated_mean_y)/estimated_sigma_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Request estimation"
      ],
      "metadata": {
        "id": "ql8q6KA7vHkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimation of the number of request per timestep : Approximation -> Day not relevant\n",
        "\n",
        "prop_coef = 6.316478902436049e-05\n",
        "tranches_reelles = []\n",
        "for i in range(96):\n",
        "  tranches_reelles.append(env.trips_data[env.trips_data['t_15min']==i]['n_trips'].sum())\n",
        "tranches = pd.DataFrame(data = {'t_15min' : env.trips_data['t_15min'].unique(),\n",
        "                     'nb_req' : [tranches_reelles[i]*prop_coef/5 for i in range(len(tranches_reelles))]})\n",
        "tranches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "a2e6e27c-152d-44a7-cf5a-83f803dfd027",
        "id": "YnBFkaUBvHkZ"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a45bee62-fccb-46eb-8a94-5b8dec4eb5be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t_15min</th>\n",
              "      <th>nb_req</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.334493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.021271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6.889118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>5.980632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5.264949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>91</td>\n",
              "      <td>17.639601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>92</td>\n",
              "      <td>16.797299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>93</td>\n",
              "      <td>15.365100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>94</td>\n",
              "      <td>14.115549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>12.926421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a45bee62-fccb-46eb-8a94-5b8dec4eb5be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a45bee62-fccb-46eb-8a94-5b8dec4eb5be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a45bee62-fccb-46eb-8a94-5b8dec4eb5be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    t_15min     nb_req\n",
              "0         0   9.334493\n",
              "1         1   8.021271\n",
              "2         2   6.889118\n",
              "3         3   5.980632\n",
              "4         4   5.264949\n",
              "..      ...        ...\n",
              "91       91  17.639601\n",
              "92       92  16.797299\n",
              "93       93  15.365100\n",
              "94       94  14.115549\n",
              "95       95  12.926421\n",
              "\n",
              "[96 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful only for the 'basic' env with 20 cars, because the requests proportions aren't the same there\n",
        "\n",
        "#request_freq = np.load('requests_count.npy')\n",
        "#tranches = pd.DataFrame(data=np.transpose([np.array(range(96)),request_freq]),columns=['t_15min','nb_req'])\n",
        "#tranches"
      ],
      "metadata": {
        "id": "a5kTA32Jnme0"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbSmoP09Jofr"
      },
      "source": [
        "## Heuristic functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic"
      ],
      "metadata": {
        "id": "PhWKgNyQtKI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plus_proche_lot(coords_voiture, time):\n",
        "  \"\"\" \n",
        "  Returns : the closest loc of a car, taking into account the time\n",
        "  \"\"\"\n",
        "  lots = numpy.array(env.lots)\n",
        "  durees = []\n",
        "  for i in range(len(lots)):\n",
        "    durees.append(duree_deplacement(coords_voiture, lots[i], time)[0])\n",
        "  return np.argmin(durees)"
      ],
      "metadata": {
        "id": "MrBBaR5dwMzM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def heuristic(state,list_wait):\n",
        "  \"\"\"\n",
        "  Basic heuristic : Make sure that every '444' cars are being repositionned, to either the closest lot or an other if we specify information\n",
        "  Doesn't relocate any other cars\n",
        "  Returns : the reposition array\n",
        "  \"\"\"\n",
        "  triplets = triplets_jobs(state)\n",
        "  reposition = [env.num_lots]*env.num_vehicles \n",
        "  if len(triplets['444']) > 0:\n",
        "    for i in range(len(triplets['444'])):\n",
        "      bool_ = True\n",
        "      for car_assign in list_wait: \n",
        "        if triplets['444'][i] == car_assign[0]:\n",
        "          bool_ = False\n",
        "          reposition[triplets['444'][i]] = car_assign[1]\n",
        "      if bool:\n",
        "        lot = plus_proche_lot(state['v_locs'][triplets['444'][i]], state['time'])\n",
        "        reposition[triplets['444'][i]] = lot\n",
        "  return np.array(reposition) "
      ],
      "metadata": {
        "id": "tH2FJCstwOFS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimized"
      ],
      "metadata": {
        "id": "rsh89WHNSyiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-processing"
      ],
      "metadata": {
        "id": "6RUhse9_soM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Processing\n",
        "\n",
        "def get_middle_areas():\n",
        "  \"\"\"\n",
        "  returns a df with the coordinate of the middle of every zone\n",
        "  \"\"\"\n",
        "  # Sampling in order to define the middle of a zone\n",
        "\n",
        "  res = []\n",
        "  x_sampling = np.linspace(env.x_range[0],env.x_range[1],500)\n",
        "  y_sampling = np.linspace(env.y_range[0],env.y_range[1],500)\n",
        "\n",
        "  for x in x_sampling:\n",
        "    for y in y_sampling:\n",
        "      res += [[x,y,env.xy_to_zone(np.array([x,y]))]]\n",
        "\n",
        "  res_ = np.array(res)\n",
        "  res_ord = pd.DataFrame(data = res_,columns=['x','y','zone'])\n",
        "  res_mid = pd.concat((res_ord.groupby([\"zone\"]).x.mean(),res_ord.groupby([\"zone\"]).y.mean()),axis=1)\n",
        "\n",
        "  return res_mid\n",
        "\n",
        "def area_depot_association(df_middle_areas):\n",
        "  \"\"\"\n",
        "  Associate a zone with a depot for every timeframe (An upgrade would be a list of the best possibilities)\n",
        "  return: Array of shape nb_timestep*nb_zones with the lot it should relocalize into.\n",
        "  \"\"\"\n",
        "  res = np.zeros((96,env.num_zones))\n",
        "  lots_coordinate = env.lots\n",
        "  for t in range(0,86400,900):\n",
        "    for i in range(env.num_zones):\n",
        "      min = 5000 #Initialize with a very high value\n",
        "      ind = 0\n",
        "      for j in range(len(lots_coordinate)):\n",
        "        temp = duree_deplacement([df_middle_areas.iloc[i]['x'],df_middle_areas.iloc[i]['y']],[lots_coordinate.iloc[j]['x'],lots_coordinate.iloc[j]['y']],t)\n",
        "        if temp<min:\n",
        "          min = temp\n",
        "          ind = j\n",
        "      res[t//900][i] = ind\n",
        "  return res\n",
        "\n",
        "middle_areas = get_middle_areas()"
      ],
      "metadata": {
        "id": "BV12Xl_mSraz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link between zones number and ranks\n",
        "\n",
        "def link_num_rank_zone():\n",
        "  \"\"\"\n",
        "  returns a df which links the zone number with their rank when they are sorted -> Makes dealing with the zone easier.\n",
        "  \"\"\"\n",
        "  df_res = pd.DataFrame(columns = ['true_zone'])\n",
        "  for i in range(env.num_zones):\n",
        "    df_res = df_res.append({'true_zone': middle_areas.iloc[i].name},ignore_index=True)\n",
        "  return df_res\n",
        "\n",
        "link_zone = link_num_rank_zone()"
      ],
      "metadata": {
        "id": "03xtTUGv9jGE"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the file containing the closest depot of every area center (need 2h to compute) \n",
        "recreate = False\n",
        "\n",
        "if recreate:\n",
        "  area_depot_by_timestep = area_depot_association(middle_areas)\n",
        "  np.save('area_depot',area_depot_by_timestep)"
      ],
      "metadata": {
        "id": "TRpJMXesTFkq"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load this file\n",
        "\n",
        "area_depot_by_timestep = np.load('area_depot.npy')\n",
        "area_depot_by_timestep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQnc_Vl4X-WV",
        "outputId": "f22e9c08-7266-48ee-bbe5-fe18bff44021"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[196., 243., 243., ...,  55., 207., 208.],\n",
              "       [196., 243., 243., ...,  55., 207., 234.],\n",
              "       [196., 243., 243., ...,  55., 207., 234.],\n",
              "       ...,\n",
              "       [ 20., 243., 207., ...,  55., 207., 234.],\n",
              "       [ 20., 243., 207., ...,  55., 207., 234.],\n",
              "       [ 20., 243., 207., ...,  55., 207., 234.]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list with every neighbour zone of a location \n",
        "\n",
        "def zone_neighbours(list_max_time):\n",
        "  df_neighbours_every_timestep = pd.DataFrame(columns = ['max_time','timestep','zone','neighbours']) \n",
        "  for max_time in list_max_time:\n",
        "    nb_zones = env.num_zones\n",
        "    for t in range(96):\n",
        "      for i in range(nb_zones):\n",
        "        temp = []\n",
        "        for j in range(nb_zones):\n",
        "          distance = duree_deplacement([env.lots.iloc[int(area_depot_by_timestep[t][i])]['x'],env.lots.iloc[int(area_depot_by_timestep[t][i])]['y']],[middle_areas.iloc[j]['x'],middle_areas.iloc[j]['y']],t*900)\n",
        "          if distance < max_time:\n",
        "            temp_ = link_zone[link_zone['true_zone'] == middle_areas.iloc[j].name].index[0]\n",
        "            temp += [temp_]\n",
        "        df_neighbours_every_timestep = df_neighbours_every_timestep.append({'max_time':max_time,'timestep':t,'zone':i,'neighbours':temp},ignore_index=True)\n",
        "  return df_neighbours_every_timestep\n",
        "\n",
        "list_max_time = [50,100,150,180,200,300,400,500,600,700,800,900]\n",
        "if recreate:\n",
        " zone_neighbours_ = zone_neighbours(list_max_time)\n",
        " zone_neighbours_.to_csv('zone_neighbours')"
      ],
      "metadata": {
        "id": "hNOSxgpjM5Hh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zxxzvaPevVe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068fe36e-08e8-4350-f46f-b659a16c6d61"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file : zone_neighbours which give the neighbours of every zone at every timestep for different values of max_time\n",
        "\n",
        "zone_neighbours = pd.read_csv('zone_neighbours.csv').drop(columns=['Unnamed: 0'])\n",
        "zone_neighbours[\"neighbours\"] = zone_neighbours[\"neighbours\"].apply(eval) #Convert the string_list into list.\n",
        "\n",
        "list_max_time = pd.unique(zone_neighbours['max_time'])"
      ],
      "metadata": {
        "id": "hsEe4WmaofdT"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file : df_neighbours_every_timestep for a max_time of 4 mins\n",
        "\n",
        "df_neighbours_every_timestep = pd.read_csv('df_neighbours_every_timestep').drop(columns=['Unnamed: 0'])\n",
        "df_neighbours_every_timestep \n",
        "\n",
        "df_neighbours_every_timestep[\"neighbours\"] = df_neighbours_every_timestep[\"neighbours\"].apply(eval) #Convert the string_list into list.\n",
        "df_neighbours_every_timestep"
      ],
      "metadata": {
        "id": "S5jqlI1sUF91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "022f3714-c1bb-4b62-9b79-71584ae39b92"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-133068a2-68ee-45d7-93c2-1d6a3ffa425d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestep</th>\n",
              "      <th>zone</th>\n",
              "      <th>neighbours</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[13, 16, 18, 19, 20, 31, 32, 41, 44, 45, 50]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 14, 59]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[1, 14, 59]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[3, 33]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>[4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5851</th>\n",
              "      <td>95</td>\n",
              "      <td>56</td>\n",
              "      <td>[55]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5852</th>\n",
              "      <td>95</td>\n",
              "      <td>57</td>\n",
              "      <td>[10, 42, 50, 57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5853</th>\n",
              "      <td>95</td>\n",
              "      <td>58</td>\n",
              "      <td>[13, 16, 19, 23, 42, 44, 58]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5854</th>\n",
              "      <td>95</td>\n",
              "      <td>59</td>\n",
              "      <td>[1, 2, 14, 48, 59]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5855</th>\n",
              "      <td>95</td>\n",
              "      <td>60</td>\n",
              "      <td>[51]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5856 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-133068a2-68ee-45d7-93c2-1d6a3ffa425d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-133068a2-68ee-45d7-93c2-1d6a3ffa425d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-133068a2-68ee-45d7-93c2-1d6a3ffa425d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      timestep  zone                                    neighbours\n",
              "0            0     0  [13, 16, 18, 19, 20, 31, 32, 41, 44, 45, 50]\n",
              "1            0     1                                   [1, 14, 59]\n",
              "2            0     2                                   [1, 14, 59]\n",
              "3            0     3                                       [3, 33]\n",
              "4            0     4                                           [4]\n",
              "...        ...   ...                                           ...\n",
              "5851        95    56                                          [55]\n",
              "5852        95    57                              [10, 42, 50, 57]\n",
              "5853        95    58                  [13, 16, 19, 23, 42, 44, 58]\n",
              "5854        95    59                            [1, 2, 14, 48, 59]\n",
              "5855        95    60                                          [51]\n",
              "\n",
              "[5856 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DECIDED NOT TO USE IT -> COMPUTATION TIME IS TOO HIGH ##\n",
        "# Pre-Processing : Neighbours(location which are attainable) for every coord for a given t and delta_t\n",
        "\n",
        "def pre_process_neighbours(t=0):\n",
        "  df_neighbours_every_coord = pd.DataFrame(columns=['t','x','y','delta_t','neighbours'])\n",
        "  delta_t = 3\n",
        "  count = 0\n",
        "  x_range = np.linspace(env.x_range[0],env.x_range[1],100)\n",
        "  y_range = np.linspace(env.y_range[0],env.y_range[1],100)\n",
        "  for x in x_range:\n",
        "    for y in y_range:\n",
        "      temp = []\n",
        "      for repo in range(env.num_lots):\n",
        "        repo_loc = env.lots.loc[repo]\n",
        "        distance_to_loc = duree_deplacement([x,y],[repo_loc['x'].item(),repo_loc['y'].item()],t*900)\n",
        "        if distance_to_loc<delta_t:\n",
        "          temp += [repo]\n",
        "      count += 1\n",
        "      print(count)\n",
        "      df_neighbours_every_coord = df_neighbours_every_coord.append({'t':t,'x':x,'y':y,'delta_t':delta_t,'neighbours':temp},ignore_index=True)\n"
      ],
      "metadata": {
        "id": "_sq3Yn_b87BG"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Documentation"
      ],
      "metadata": {
        "id": "ipX9LV7p5yng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the heuristic Employed for repositionning**\n",
        "\n",
        "General Idea : We'r going to look into the future and try to know where we would like to have cars in the future. We'll use delta_t for the timeframe between the present and the future.\n",
        "\n",
        "They are 3 different parts:\n",
        "* I) Retrieve the possibilities of replacement for the future considered.\n",
        "* II) Find a good possibility among all these possibilities of replacement.\n",
        "* III) Use this possibility to produce the right vector 'q_reloc' for the environnment.\n",
        "\n",
        "**I)** \n",
        "\n",
        "First we need to gather, all the cars which are going to be available at t+delta_t (right away): These are the car which won't be in '323','234' or '344'. We also need the time left they have to reposition themselves. \n",
        "From there,there are different strategies, We can either:\n",
        "* Compute the distance between the position of the cars (position of destination for cars in '344','104'..) and every location to know where they can relocate themselves. -> Big computation time required.\n",
        "We can also restrict it to only a few locations we've selected, as an example, 1 location for every zone which is defined by the closest location of its center. (In the code : Exact = True)\n",
        "* Get the zones in which, our cars are and precompute the location which are atteignable from a zone center (considered to be a zone) and we associate then every car with the zone center in which they are in. -> Pre-computation which isn't too hard to do, but a little approximation on the results. (In the code : Exact = False)\n",
        "* We create a grid, and for every point of this grid we calculate the locations which are reachable, what's left to do, is to associate the coord x,y of a position to a point of the grid. -> Enormous computation time (cause we must do a grid for every timestep/day/delta_t) so we decided not to choose this option.\n",
        "When a car has not enough time to atteign a relocation spot (exception for '444'), we don't consider it for a relocation, but we should still update the map_value according to the position it'll be in (What we don't do currently).\n",
        "\n",
        "We now have the possibilities of replacement for every car which are going to be available ('available' as we've defined it).\n",
        "\n",
        "**II)**\n",
        "\n",
        " The Idea here, is to find a good replacement solution. \n",
        "What would be an optimal solution? \n",
        "We first need to gather the amount of requests ('tranches') we will receive at a given timestep and the density of requests starting point for every zone. From there we can create a map ('zone_density or map_density'), which will gives us the information : \n",
        "**How many requests will there be in the future in every zone?**\n",
        "\n",
        "Then we can start evaluating a possibility.\n",
        "We've defined a metric with multiple hyperparameters to evaluate the repositions for a state on the basis of the map_density. The idea is that we're going to evaluate how many requests we will be able to do a t+delta_t and we're going to also take into account how much distance we'll travel with these repositionnments. \n",
        "\n",
        "The function we use is : $\\frac{((nb\\_request\\_futur - (how many requests won't be possible to take))/nb\\_request\\_futur)}{((1+travel\\_distance)^{(distance\\_importance/((nb\\_request)^{(nb\\_request\\_importance)}))}}$\n",
        "\n",
        "The numerator tells us how many requests we are going to do at t+delta_t,\n",
        "The denominator weights the distance we'll traval and take into account that if there are many requests if may be better to travel a little bit more. There are 2 hyper-parameters : nb_request_importance and distance_importance.\n",
        "Related functions in the code : travel_distance,calcul_map_value.\n",
        "\n",
        "\n",
        "We now have a way of assessing a possibility : What we could do is try exhaustivly every possibility (function:'exhaustive search'). But in a lot of cases the computation time is way too high! So we tried to develop a heuristic, which **looks for a good solution in a reasonable amount of time**.\n",
        "\n",
        "First to simplify the problem, we tried to create cluster of cars that we could consider independent from on another, to do so, we considered that if 2 cars had a reposition location in common then they should belong to the same cluster. A hypothese which is done here, is that we consider that when a request pop, it's only atteignable by a single cluster which is HUGE. Still it simplifies the problem and gives good results so it's justifiable.\n",
        "\n",
        "To do this clustering we used a Graph class and a fonction which could give us the principal components of this graph. The technical part isn't too interesting, what's interesting is that we now have clusters of cars whose repositions are considered independant from one another.\n",
        "\n",
        "We can now use these clusters to find an good solution. The method employed is as follow : \n",
        "If there's only 1 element we test all the possibilities and return the best one.\n",
        "If there's more than 1 element, we rank the zones from best to worst by simulation that there would be a car in this zone, using a geometric law we take on of these possibilities, assign the closest car to this possibility and continue this process without the car we've assigned and by update the density_map!\n",
        "We iterate this Operation a certain amount of time and we take the best score. \n",
        "\n",
        "An additition to that, we've coded, is to also add the 'no replacement' option because it is oft a good option to consider.\n",
        "\n",
        "\n",
        "We now have a replacement for every car.\n",
        "\n",
        "III) \n",
        "\n",
        "What's left to do is to filter, meaning, we must, for every car in '444' or '044' reassign them accordingly and for the rest we store the result in a wait_list, which is going to be updated any time we apply this algorithm.\n",
        "\n",
        "What's left to discuss, is at what frequency we apply our algorithm, an approach, which seems good, is to apply every 3 min our algorithm with a delta_t of 3 min and every 15 min instead with a delta_t of 15 min to see further into the future.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gHkUU_1X54pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Heuristic functions"
      ],
      "metadata": {
        "id": "SOdlaLeetAz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Key functions"
      ],
      "metadata": {
        "id": "D_cPuEDQueaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Graph class to find the connexe component of our \"possibilities\" : Allow to consider certain set of choices as independant.\n",
        "\n",
        "class Graph():\n",
        "    def __init__(self, S):\n",
        "        self.S = S\n",
        "        self.plus = {}\n",
        "        self.moins = {}\n",
        "        self.CFC = {}\n",
        "        for e in S.keys(): self.CFC[e] = False\n",
        "    def suiv(self, som):\n",
        "        return self.S[som]\n",
        "    def pred(self, som):\n",
        "        t = []\n",
        "        for e in self.S.keys():\n",
        "            if som in self.S[e]:\n",
        "                t.append(e)\n",
        "        return t\n",
        "    def sommets(self):\n",
        "        return list(self.S.keys())\n",
        "    def init_mark(self):\n",
        "        for e in self.S.keys():\n",
        "            self.plus[e] = self.moins[e] = False\n",
        "\n",
        "def dfs_suiv(graph, s):\n",
        "    \"\"\"\n",
        "        Parcours DFS des suivants a partir du \n",
        "        sommet s\n",
        "    \"\"\"\n",
        "    if graph.plus[s]: return\n",
        "    graph.plus[s] = True\n",
        "    for e in graph.suiv(s):\n",
        "        dfs_suiv(graph, e)\n",
        "        \n",
        "def dfs_pred(graph, s):\n",
        "    \"\"\"\n",
        "        Parcours DFS des precedents a partir du \n",
        "        sommet s\n",
        "    \"\"\"\n",
        "    if graph.moins[s]: return\n",
        "    graph.moins[s] = True\n",
        "    for e in graph.pred(s):\n",
        "        dfs_pred(graph, e)\n",
        "\n",
        "def CFC(g):\n",
        "    composantes = []\n",
        "    for s in g.sommets(): # pour chaque sommet\n",
        "        if g.CFC[s]:\n",
        "            continue # S'il appartient deja a un CFC on le saute\n",
        "        g.init_mark()\n",
        "        dfs_suiv(g,s) # propagation des plus\n",
        "        dfs_pred(g,s) # propagation des moins\n",
        "        res = []\n",
        "        for e in g.sommets():\n",
        "            if g.plus[e] == g.moins[e] == True:\n",
        "                res.append(e)\n",
        "                g.CFC[e] = True\n",
        "        composantes.append(res)\n",
        "    g.init_mark()\n",
        "    return composantes"
      ],
      "metadata": {
        "id": "GtvA0G1Ex6UB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heuristique Optimized\n",
        "\n",
        "# Major possibilities of improvements : Rajouter l'aspect -> Des requetes vont intervenir entre t et t+delta_t / Classification des requetes en bonnes ou mauvaises.\n",
        "\n",
        "def available_cars(state,delta_t,time,jobs): #May be interessant to store these durations somehow and update them every time needed.\n",
        "  \"\"\"\n",
        "  returns the available cars at t+delta_t with the timeleft they have.\n",
        "  \"\"\"\n",
        "  available_cars = []\n",
        "  for car in jobs['044']+ jobs['444']:\n",
        "    available_cars += [[car,delta_t]]\n",
        "  for car in jobs['344']+ jobs['104'] :\n",
        "    time_to_end = duree_deplacement(state['v_locs'][car],state['v_job_locs'][car][0][1],time)\n",
        "    if time_to_end < delta_t:\n",
        "      available_cars += [[car,delta_t - time_to_end[0]]]\n",
        "  for car in jobs['234']:\n",
        "    time_to_end = duree_deplacement(state['v_locs'][car],state['v_job_locs'][car][0][1],time) + duree_deplacement(state['v_job_locs'][car][1][0],state['v_job_locs'][car][1][1],time)\n",
        "    if time_to_end < delta_t:\n",
        "      available_cars += [[car,delta_t - time_to_end[0]]]\n",
        "  for car in jobs['323']:\n",
        "    time_to_end = duree_deplacement(state['v_locs'][car],state['v_job_locs'][car][0][1],time) + duree_deplacement(state['v_job_locs'][car][0][1],state['v_job_locs'][car][1][0],time) + duree_deplacement(state['v_job_locs'][car][1][0],state['v_job_locs'][car][1][1],time) + duree_deplacement(state['v_job_locs'][car][2][0],state['v_job_locs'][car][2][1],time)  \n",
        "    if time_to_end < delta_t:\n",
        "      available_cars += [[car,delta_t - time_to_end[0]]]\n",
        "  return available_cars\n",
        "\n",
        "def loc_reachable(car,state,time_left,time,exact=False): #\n",
        "  \"\"\"\n",
        "  Exact: we calculate the true locations which are accessible by iterating if True,else, we consider that the car is at the same position that the center of its zone, it allows to reduce the computation time.\n",
        "  returns the location which are reachable for a car in less than 'time_left' time.\n",
        "  The location are only taken from the location which are the closest to a zone center!\n",
        "  \"\"\"\n",
        "  pos_car = state['v_locs'][car]\n",
        "  list_res = []\n",
        "  jobs = triplets_jobs(state)\n",
        "  # Could add the closest location for 444 but we can also do it at the end!\n",
        "  # Also add the current location for 044 cause the current location may not be in the set of location we consider.\n",
        "  if car in jobs['044']:\n",
        "    test = env.lots[(env.lots['x']==state['v_locs'][car][0]) & (env.lots['y']==state['v_locs'][car][1])]\n",
        "    if len(test) != 0:\n",
        "      list_res += [test.index[0]]\n",
        "  if car in jobs['104']:\n",
        "    if exact:\n",
        "      list_res += [plus_proche_lot(state['v_job_locs'][car][0][1],state['time'])]\n",
        "    else:\n",
        "      true_zone = env.xy_to_zone(pos_car)\n",
        "      car_zone = link_zone[link_zone['true_zone']==true_zone].index[0]\n",
        "      closest_neigh = area_depot_by_timestep[int(time//900)][car_zone]\n",
        "      list_res += [closest_neigh]\n",
        "  if exact:\n",
        "    for loc in area_depot_by_timestep[int(time//900)]:\n",
        "      if duree_deplacement(pos_car,[env.lots.loc[loc]['x'],env.lots.loc[loc]['y']],time) < time_left:\n",
        "        list_res += [loc]\n",
        "  else:\n",
        "    true_zone = env.xy_to_zone(pos_car)\n",
        "    car_zone = link_zone[link_zone['true_zone']==true_zone].index[0]\n",
        "    time = int(time//900)\n",
        "    absolute_difference_function = lambda list_value : abs(list_value - time_left)\n",
        "    time_left_approx = min(list_max_time, key=absolute_difference_function)\n",
        "    neighbours = zone_neighbours[(zone_neighbours['max_time']==time_left_approx) & (zone_neighbours['timestep']==time) & (zone_neighbours['zone']==car_zone)]['neighbours'].item()\n",
        "    loc_neighbours = []\n",
        "    for neighbour in neighbours:\n",
        "      loc_neighbours += [area_depot_by_timestep[int(time//900)][neighbour]]\n",
        "    list_res += loc_neighbours\n",
        "  return list(set(list_res))#supress duplicates\n",
        "\n",
        "def zone_possibility(state,available_cars,time): \n",
        "  \"\"\"\n",
        "  returns every possibility of relocation for every cars according to the time left they have before t+delta_t.\n",
        "  Needs a high computation time.\n",
        "  \"\"\"\n",
        "  res = []\n",
        "  for car,time_left in available_cars:\n",
        "    loc_reach = loc_reachable(car,state,time_left,time)\n",
        "    if loc_reach != []:\n",
        "      res += [[car,loc_reach]]\n",
        "  return res\n",
        "\n",
        "def map_density(state,nb_estimated_request,delta_t):\n",
        "  \"\"\"\n",
        "  returns the 'map density' for a given state at t+delta_t (which zone have the most requests) and multiply these values by the number of requests to make it easier to analyse it\n",
        "  \"\"\"\n",
        "  time = int((state['time']+delta_t)//900)\n",
        "  dow = state['dow']\n",
        "  df_temp = env.trips_data[(env.trips_data['dow']==dow) & (env.trips_data['t_15min']==0)].groupby('puzone').n_trips.sum().reset_index()\n",
        "  total_amount_req = df_temp.n_trips.sum()\n",
        "  df_temp['n_trips'] = df_temp['n_trips']/total_amount_req*nb_estimated_request\n",
        "  return df_temp \n",
        "\n",
        "def travel_distance(state,assignments):\n",
        "  \"\"\"\n",
        "  returns the additionnal travel distance caused by the repositionnments we consider\n",
        "  \"\"\"\n",
        "  total_distance = 0\n",
        "  jobs = triplets_jobs(state)\n",
        "  for assign in assignments:\n",
        "    car = assign[0]\n",
        "    loc = assign[1]\n",
        "    if car in jobs['044'] + jobs['444']: #Take the car position as a reference\n",
        "      total_distance += dist_manhattan(state['v_locs'][car],[env.lots.loc[loc]['x'],env.lots.loc[loc]['y']])\n",
        "    elif car in jobs['344'] + jobs['104']: #Take the last loc it will be in\n",
        "      total_distance += dist_manhattan(state['v_job_locs'][car][0][1],[env.lots.loc[loc]['x'],env.lots.loc[loc]['y']])\n",
        "    elif car in jobs['234']: #Take the last loc it will be in\n",
        "      total_distance += dist_manhattan(state['v_job_locs'][car][1][1],[env.lots.loc[loc]['x'],env.lots.loc[loc]['y']])\n",
        "    else:\n",
        "      total_distance += dist_manhattan(state['v_job_locs'][car][2][1],[env.lots.loc[loc]['x'],env.lots.loc[loc]['y']])\n",
        "  return total_distance\n",
        "\n",
        "def list_into_dict(zone_possibility):\n",
        "  \"\"\"\n",
        "  Converts the 'zone_possibility' list into a dict, to make the search of the \n",
        "  \"\"\"\n",
        "  dict_res = {}\n",
        "  for ele in zone_possibility:\n",
        "    dict_res[ele[0]] = ele[1]\n",
        "  return dict_res\n",
        "\n",
        "def calcul_map_value(state,assignments,zone_density,nb_request,distance_importance=0.04,true_reach=False): #assignements : list des zones assignements.\n",
        "  \"\"\"\n",
        "  Calculate for a state and an assigment set (example: car 1 -> 234, car 2 -> 165,...), the map value of this configuration in order to compare it with other map value.\n",
        "  This map_value is more detailled in the documentation.\n",
        "  \"\"\"\n",
        "  travel_distance_ = travel_distance(state,assignments)\n",
        "  zone_density_ = zone_density.copy()\n",
        "  time = int(state['time']//900)\n",
        "  for assign in assignments:\n",
        "    coord = env.lots.iloc[int(assign[1])]\n",
        "    coord = np.array([coord['x'],coord['y']])\n",
        "    zone_coord = env.xy_to_zone(coord)\n",
        "    zone_coord = link_zone[link_zone['true_zone']==zone_coord].index[0]\n",
        "    # Get every neighbour of the relocalisation we intend to make.\n",
        "    neighbours = df_neighbours_every_timestep[(df_neighbours_every_timestep['timestep']==time) & (df_neighbours_every_timestep['zone']==zone_coord)]['neighbours'].item()\n",
        "    nb_neighbours = len(neighbours)\n",
        "    neighbours = list(neighbours) \n",
        "    for neighbour in neighbours:\n",
        "      zone_density_.loc[neighbour, 'n_trips'] = max(0,zone_density_.loc[neighbour, 'n_trips']-1/(nb_neighbours))\n",
        "  if true_reach: #True_reach means that we don't consider the distance we travel as a negative factor, useful for the analysis\n",
        "    map_value = (nb_request - zone_density_['n_trips'].sum())/nb_request\n",
        "  else:\n",
        "    map_value = (nb_request - zone_density_['n_trips'].sum())/nb_request/((1+travel_distance_)**(distance_importance/((nb_request)**1.1))) #Test différents valeurs\n",
        "  return map_value\n",
        "\n",
        "def cluster(zone_possibility):\n",
        "  \"\"\"\n",
        "  Create a cluster using the connexe component of the graph 'zone_possibility'.\n",
        "  \"\"\"\n",
        "  graph = graphe_from_zone_possibility(zone_possibility)\n",
        "  g = Graph(graph)\n",
        "  cfcs = CFC(g)\n",
        "  list_total = []\n",
        "  for list_ in cfcs:\n",
        "    list_total += [[-item-1 for item in list_ if item < 0]] #From the negative numbers which are cars, we gather there true values.\n",
        "  return list_total\n",
        "\n",
        "def graphe_from_zone_possibility(zone_possibility):\n",
        "  \"\"\"\n",
        "  Using the zone_possibility, we try to know if some cars have common reposition possibilities, if it's the case, they belong to the same cluster!\n",
        "  returns: A graph on which, we can use the PCD algorithm.\n",
        "  \"\"\"\n",
        "  graphe = {}\n",
        "  for car_reloc in zone_possibility:\n",
        "    car = - car_reloc[0] - 1 #Car are negative numbers in our graphe\n",
        "    graphe[car] = []\n",
        "    for reloc in car_reloc[1]:\n",
        "      if reloc not in graphe.keys():\n",
        "        graphe[reloc] = [car] #Every car and relocation are a key in this graph\n",
        "      else:\n",
        "        graphe[reloc] += [car]\n",
        "      graphe[car].append(reloc)\n",
        "  return graphe"
      ],
      "metadata": {
        "id": "Fcd95kvBWPuh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Search of optimal solution"
      ],
      "metadata": {
        "id": "YFJiQwwzup-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exhaustive_search_aux(nb_estimated_request,state,time,zone_density,list_datapoint,assignments,step,step_max):\n",
        "  \"\"\"\n",
        "  Auxilary function of exhaustive_search\n",
        "  \"\"\"\n",
        "  if step == step_max:\n",
        "    return [[calcul_map_value(state,assignments,zone_density,nb_estimated_request),assignments]]#[calcul_map_value(0,assignments,0,0,0,0),assignments]\n",
        "  else:\n",
        "    l = []\n",
        "    for possibility in list_datapoint[0][1]:\n",
        "      l += [exhaustive_search_aux(nb_estimated_request,state,time,zone_density,list_datapoint[1:],assignments + [[list_datapoint[0][0],possibility]],step + 1,step_max)]\n",
        "    return(max(l))\n",
        "\n",
        "def exhaustive_search(state,zone_possibility,time,zone_density,nb_estimated_request):\n",
        "  \"\"\"\n",
        "  Exhaustive research of every replacement possibilities (every combinaison of cars replacements within the timeframe delta_t)\n",
        "  return: the best result found : (score,[replacements for every car])\n",
        "  \"\"\"\n",
        "  list_datapoint = zone_possibility\n",
        "  best_res = exhaustive_search_aux(nb_estimated_request,state,time,zone_density,list_datapoint,assignments=[], step=0,step_max=len(list_datapoint))\n",
        "  return best_res\n",
        "\n",
        "def apply_a_car_zone_density(state,zone_density,zone_to_apply):\n",
        "  \"\"\"\n",
        "  Function which apply the impact of a car to the zone_density.\n",
        "  returns : the new zone_density (we diminished the weights of every zone which are neighbours from the zone we consider)\n",
        "  \"\"\"\n",
        "  time = int(state['time']//900)\n",
        "  zone_density_ = zone_density.copy()\n",
        "  coord = env.lots.iloc[int(zone_to_apply)]\n",
        "  coord = np.array([coord['x'],coord['y']])\n",
        "  zone_coord = env.xy_to_zone(coord)\n",
        "  zone_coord = link_zone[link_zone['true_zone']==zone_coord].index[0]\n",
        "  neighbours = df_neighbours_every_timestep[(df_neighbours_every_timestep['timestep']==time) & (df_neighbours_every_timestep['zone']==zone_coord)]['neighbours'].item()\n",
        "  nb_neighbours = len(neighbours)\n",
        "  neighbours = list(neighbours)\n",
        "  for neighbour in neighbours:\n",
        "    zone_density_.loc[neighbour, 'n_trips'] = max(0,zone_density_.loc[neighbour, 'n_trips']-1/(nb_neighbours))\n",
        "  return zone_density_\n",
        "\n",
        "def no_replacement(state,zone_possibility_,exact=False):\n",
        "  \"\"\"\n",
        "  From a state, recuperate the 'no replacement' possibility so that we always consider this possibility\n",
        "  returns : [[[car,(current or closest relocalisation)]] for every car which are available]\n",
        "  \"\"\"\n",
        "  # Take only cars which are considered available.\n",
        "  jobs = triplets_jobs(state)\n",
        "  available_cars = list(zone_possibility_.keys()) #Get the available cars\n",
        "  res = []\n",
        "  for car in jobs['044']:\n",
        "    res += [[car,env.lots[(env.lots['x']==state['v_locs'][car][0]) & (env.lots['y']==state['v_locs'][car][1])].index[0]]] #plus_proche_lot(state['v_locs'][car],state['time'])\n",
        "  for car in jobs['444']:\n",
        "    if car in available_cars:\n",
        "      res += [[car,plus_proche_lot(state['v_locs'][car],state['time'])]]\n",
        "  for car in jobs['104'] + jobs['344']:\n",
        "    if car in available_cars:\n",
        "      if exact:\n",
        "        res += [[car,plus_proche_lot(state['v_job_locs'][car][0][1],state['time'])]]\n",
        "      else:\n",
        "        pos_car = state['v_locs'][car]\n",
        "        true_zone = env.xy_to_zone(pos_car)\n",
        "        car_zone = link_zone[link_zone['true_zone']==true_zone].index[0]\n",
        "        closest_neigh = area_depot_by_timestep[int(state['time']//900)][car_zone]\n",
        "        res += [[car,closest_neigh]]\n",
        "  for car in jobs['234']:\n",
        "    if car in available_cars:\n",
        "      res += [[car,plus_proche_lot(state['v_job_locs'][car][1][1],state['time'])]]\n",
        "  for car in jobs['323']:\n",
        "    if car in available_cars:\n",
        "      res += [[car,plus_proche_lot(state['v_job_locs'][car][2][1],state['time'])]]\n",
        "  return res\n",
        "\n",
        "def assign_from_cluster_aux(cluster,state,zone_possibility_,time,zone_density,nb_estimated_request,list_in_progress):\n",
        "  \"\"\"\n",
        "  Auxilary recursive function of assign_from_cluster \n",
        "  For a given cluster : we take the 'best' zones, apply a geometric law to it(add a random part) and go on like that until there's only 1 element.\n",
        "  We iterate this operation a certain amount of time, and we take the best result.\n",
        "  \"\"\"\n",
        "  jobs = triplets_jobs(state)\n",
        "  if cluster == [] :\n",
        "    return list_in_progress\n",
        "  else:\n",
        "    # Get every accessible zone\n",
        "    possible_zone = []\n",
        "    for car in cluster:\n",
        "      for loc in zone_possibility_[car]:\n",
        "        if loc not in possible_zone:\n",
        "          possible_zone += [loc]\n",
        "\n",
        "    # Define the best zones -> sample from it with a descending probability\n",
        "    list_zone_value = []\n",
        "    for loc in possible_zone:\n",
        "      list_zone_value += [[calcul_map_value(state,[[0,loc]],zone_density,nb_estimated_request),loc]]\n",
        "    list_zone_value.sort(reverse=True)\n",
        "    chosen_loc_ind = min(np.random.geometric(p=0.5),len(list_zone_value)-1)\n",
        "    chosen_loc = list_zone_value[chosen_loc_ind][1]\n",
        "\n",
        "    # Get the closest car -> associate it\n",
        "    distance_to_chosen_loc = []\n",
        "    for car in cluster:\n",
        "      if car in jobs['044'] + jobs['444']:\n",
        "        distance_to_chosen_loc += [[dist_manhattan(state['v_locs'][car],[env.lots.loc[chosen_loc]['x'],env.lots.loc[chosen_loc]['y']]),car]]\n",
        "      elif car in jobs['344'] + jobs['104']:\n",
        "        distance_to_chosen_loc += [[dist_manhattan(state['v_job_locs'][car][0][1],[env.lots.loc[chosen_loc]['x'],env.lots.loc[chosen_loc]['y']]),car]]\n",
        "      elif car in jobs['234']:\n",
        "        distance_to_chosen_loc += [[dist_manhattan(state['v_job_locs'][car][1][1],[env.lots.loc[chosen_loc]['x'],env.lots.loc[chosen_loc]['y']]),car]]\n",
        "      else:\n",
        "        distance_to_chosen_loc += [[dist_manhattan(state['v_job_locs'][car][2][1],[env.lots.loc[chosen_loc]['x'],env.lots.loc[chosen_loc]['y']]),car]]\n",
        "    chosen_car = min(distance_to_chosen_loc)[1]\n",
        "\n",
        "    # The car in list_in_progress/ supress it from cluster \n",
        "    cluster.remove(chosen_car)\n",
        "    zone_density = apply_a_car_zone_density(state,zone_density,chosen_loc)\n",
        "    return assign_from_cluster_aux(cluster,state,zone_possibility_,time,zone_density,nb_estimated_request,list_in_progress+[[chosen_car,chosen_loc]])\n",
        "\n",
        "def assign_from_cluster(clusters,state,zone_possibility_,time,zone_density,nb_estimated_request,nb_try):\n",
        "  \"\"\"\n",
        "  From a cluster set, for every cluster : we either take the best possibility if there's only 1 element, or we apply the auxilary function (nb_try time in order to try different interesting configurations).\n",
        "  \"\"\"\n",
        "  # Deal with the cluster where there's only one element first because they aren't random!\n",
        "  list_res = []\n",
        "  list_aux = []\n",
        "  ele_to_supress = []\n",
        "  for cluster in clusters:\n",
        "    if len(cluster)==1:\n",
        "      ele_to_supress += [cluster]\n",
        "      car = cluster[0]\n",
        "      list_reloc = []\n",
        "      for loc in zone_possibility_[car]:\n",
        "        list_reloc += [[calcul_map_value(state,[[car,loc]],zone_density,nb_estimated_request),[car,loc]]]\n",
        "      assign = max(list_reloc)\n",
        "      list_aux += [assign[1]]\n",
        "  for ele in ele_to_supress:\n",
        "    clusters.remove(ele)\n",
        "  \n",
        "  # Deal with the rest\n",
        "  for try_ in range(nb_try):\n",
        "    list_aux_ = list_aux.copy()\n",
        "    for cluster in clusters:\n",
        "      cluster_ = cluster.copy()\n",
        "      list_aux_ += assign_from_cluster_aux(cluster_,state,zone_possibility_,time,zone_density,nb_estimated_request,list_in_progress = [])\n",
        "    list_res += [list_aux_]\n",
        "  \n",
        "  # Add the option : No replacement\n",
        "  list_res += [no_replacement(state,zone_possibility_)]\n",
        "\n",
        "  # Take the best solution we'v found so far.\n",
        "  list_map_value = []\n",
        "  for possibility in list_res:\n",
        "    list_map_value += [[calcul_map_value(state,possibility,zone_density,nb_estimated_request),possibility,calcul_map_value(state,possibility,zone_density,nb_estimated_request,true_reach=True)]]\n",
        "  \n",
        "  return max(list_map_value)"
      ],
      "metadata": {
        "id": "3uWYyU-C-8c6"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Heuristic"
      ],
      "metadata": {
        "id": "EKKBZxTVuvAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heuristic\n",
        "\n",
        "def heuristique_optimized(state,delta_t=900):\n",
        "  \"\"\"\n",
        "  Apply the optimized_heuristic algorithm\n",
        "  \"\"\"\n",
        "  time = state['time']\n",
        "  ##\n",
        "  # Retrive jobs triplets \n",
        "  jobs = triplets_jobs(state)\n",
        "\n",
        "  # Retrive available cars at t + delta_t with the amount of time left!\n",
        "  available_cars_ = available_cars(state,delta_t,time,jobs)\n",
        "\n",
        "  # Retrive possibiliies for every of these cars (depending on the zone they're in) -> Dict?\n",
        "  zone_possibility_ = zone_possibility(state,available_cars_,time) #Could create a 2nd one for the graph.\n",
        "  \n",
        "  # Create a cluster of cars\n",
        "  clusters = cluster(zone_possibility_)\n",
        "  \n",
        "  # Compute the amount of request we expect to have in the next 15 minutes.\n",
        "  nb_estimated_request = tranches[tranches['t_15min']==time//900].nb_req.item()\n",
        "\n",
        "  # For a given timestep : Retrieve map with nb_cars and density for every zone (estimés à t + delta_t). -> DataFrame avec pour chaque zone un poids.\n",
        "  zone_density = map_density(state,nb_estimated_request,delta_t)\n",
        "  \n",
        "  # A partir des possibilités et de cette map : appel function optimisation qui test toutes les possibilités et renvoie la meilleure.\n",
        "  best_possibility_ = assign_from_cluster(clusters,state,list_into_dict(zone_possibility_),time,zone_density,nb_estimated_request,2)\n",
        "  print(best_possibility_)\n",
        "  #best_possibility_ = exhaustive_search(state,zone_possibility_,time,zone_density,nb_estimated_request)[0]\n",
        "  #print(best_possibility_)\n",
        "\n",
        "  # A partir des zones de replacement -> vecteur de relocalisation uniquement pour les '044'! list_wait pour les autres qui les assigneras plus tard!\n",
        "  relocalisation_vector = np.array([env.num_lots]*env.num_vehicles)\n",
        "  list_wait_reassignment = []\n",
        "  for assign in best_possibility_[1]:\n",
        "    car = assign[0]\n",
        "    loc = assign[1]\n",
        "    if (car in jobs['044'] and  (state['v_locs'][car] != [env.lots.loc[loc, 'x'],env.lots.loc[loc, 'y']]). all())or (car in jobs['444']):\n",
        "      relocalisation_vector[car] = loc\n",
        "    elif car in jobs['104'] + jobs['344'] + jobs['323']:\n",
        "      list_wait_reassignment += [[car,loc]]\n",
        "  # Deal with 444 cars which weren't assign yet.\n",
        "  for car in jobs['444']:\n",
        "    if relocalisation_vector[car] == env.num_lots:\n",
        "      relocalisation_vector[car] = plus_proche_lot(state['v_locs'][car],time)\n",
        "  \n",
        "  return relocalisation_vector,list_wait_reassignment"
      ],
      "metadata": {
        "id": "t9H5DKbzumJ-"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additionnal function"
      ],
      "metadata": {
        "id": "ssFTnV6-L1iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_state(state,req_assign):\n",
        "  \"\"\"\n",
        "  This function allows to take into account which car were assigned to change their status!\n",
        "  Change the 'state' used for the heuristic according to the requests that we've assigned so far.\n",
        "  returns : the changed state\n",
        "  \"\"\"\n",
        "  state_ = state.copy()\n",
        "  for i,car in enumerate(req_assign):\n",
        "    if car != env.num_vehicles:\n",
        "      job_car = list(state_['v_jobs'][car])\n",
        "      if job_car == [3,4,4]:\n",
        "        state_['v_jobs'][car] = np.array([3,2,3])\n",
        "        state_['v_job_locs'][car][1] = state_['request_locs'][i][0]\n",
        "        state_['v_job_locs'][car][2] = state_['request_locs'][i][1]\n",
        "      elif job_car != [4,4,4]:\n",
        "        state_['v_jobs'][car] = np.array([2,3,4])\n",
        "        state_['v_job_locs'][car][0] = state_['request_locs'][i][0]\n",
        "        state_['v_job_locs'][car][1] = state_['request_locs'][i][1]\n",
        "  return state_"
      ],
      "metadata": {
        "id": "O_oJDAaQL5_E"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCDN3XuWKx1T"
      },
      "source": [
        "## Q learning algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentation"
      ],
      "metadata": {
        "id": "OepW7p7IcYCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DeepQ Network**\n",
        "\n",
        "The general idea of this part, is to make a neural network learn the request assignment tasks. They are 2 different part that would be difficult to optimize with a classical heuristic : \n",
        "* In a time where they are many requests, how do we choose not to assign a request or to wait because we think that a better request could pop up and that it's worth waiting.\n",
        "* In our problem, when we assign a request, they are 2 metrics to assess how 'close' is the car : The distance and the time. These 2 metrics aren't even sufficient on their own, it may be be more interesting to assign a car which is in a cold zone than a hot zone car even if the distance if greater. \n",
        "\n",
        "For these reasons, we want to delegate this task to a neural network.\n",
        "\n",
        "***A little recall on the DeepQLearning Algorithm :***\n",
        "\n",
        "We want to estimate the Q_value of a state and an action, which represents the cumulative reward we imagine having by taking this action in this state. \n",
        "\n",
        "To do so, we create 2 Networks (2 for stationnary,converge reasons), which will learn to approximate these Q_values. There's the Q_net and the target_net. Only the Q_net will learn, the target net will be a timeshifted copy of the Q_net. Our goal is to minimize the function for given states and actions: \n",
        "\n",
        "$f = Q\\_value_{Q\\_net} - (reward + discount\\_rate*Q\\_value_{\"target\"})$\n",
        "\n",
        "The first term reprensents the Q_value for a state S. The second term, the Q_value for that state but considering the new state S' that has been obtained by applying the action a to the environment. It should be homogenious if our network could correctly approximate the Q_values of an environment. \n",
        "This function tells us how homegenize our networks are, what we want is to minimize it! \n",
        "\n",
        "That's not all, we want that our network be able to approximate the Q_values of many different potential good setup (action in a state) in order to assess correctly a lot of possibly good possibilities. The exploration strategy rules this part. \n",
        "\n",
        "***Application/Modelization in our problem***\n",
        "\n",
        "In our environnement, the decision we take is to assign, for a request, if a car is going to take the request and if it's the case then which car.\n",
        "The problem is that with the DeepQ Algorithm we need an estimation of the Q_value of an action in a state (an action being the whole set of assignment and reposition but considering that we always reposition the same way, we can do the hypotheses that it's only dependent from the set of assignments in a given state). But we don't want to asset every combination of assignments in a given state. To linder that, our network is gonna make a decision for every request and are then going to aggregate the prediction (by a mean) to get the Q_value of a given state and a given action.\n",
        "\n",
        "On top of that, we must be sure that a state has a request when we consider it for the learning process, when it's not the case, we just iterate and accumulate the rewards until we get such a state.\n",
        "\n",
        "A problem that occur is the overlapping in ourcase : our network expresses Q_values for every request, it is possible that a car be the best choice for 2 different requests. In that case, we applied the rule 'first arrived, first served', it could be improve but it wouldn't be a major improvment of our network.\n",
        "\n",
        "Our network is then going to make an estimation of the value of assigning a car to a request or not assigning a car, so there are nb_cars + 1 possible actions.\n",
        "\n",
        "***Inputs***\n",
        "\n",
        "As an input we have 2 different tensor, one with global features and one with features specific to every car.\n",
        "General Tensor features : \n",
        "* Day of the week (one hot encoded)\n",
        "* Time\n",
        "* Request time\n",
        "* Number of available cars ('044', '104, '444')\n",
        "* Number of requests in this state\n",
        "* Number of requests in the last 15 minutes\n",
        "* Requests data (coordinates,distance,estimated time of trip)\n",
        "\n",
        "Specificity of the network for Car Tensor features : permutability of cars (the weigth of the network at the first layer should be all the same).\n",
        "Car Tensor features : \n",
        "* Coordinates\n",
        "* Distance to request\n",
        "* Time to request\n",
        "* Probability that request be done in less than 5 minutes.\n",
        "\n",
        "***Masking***\n",
        "\n",
        "Cars which are in '323' and '234' can't take more requests so no request should be assign to them, we mask them in our network so that it be impossible to choose them.\n",
        "We've also tried a few approachs where we mask the cars which have a too low probability of making the trip to the request in less than 5 minutes. (A typical value is 0.5).\n",
        "\n",
        "***Memory***\n",
        "\n",
        "We're not going into details of why it is interessting to have a memory for the DeepQ Algorithm.\n",
        "In our case, we transform the state dict and store only informations which are going to be useful into different lists.\n",
        "\n",
        "***Specificity of our learning process***\n",
        "\n",
        "The discount_rate (coefficients which make futur reward less 'important') could depend on the time in our problem, the 2 approachs are valid.\n",
        "\n",
        "We've tried SARSA and deepQ. \n",
        "\n",
        "We learn every few 30-50 steps with a batch_size of 32 normally.\n",
        "\n"
      ],
      "metadata": {
        "id": "5HiH72P4chol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepQ"
      ],
      "metadata": {
        "id": "TPjtH1CSce0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "4trOxLIjAsBq"
      },
      "outputs": [],
      "source": [
        "# Memory\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, device):\n",
        "        self.max_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "\n",
        "        self.state_cars_memory = [] #Stores every characterics which are proper to cars\n",
        "        self.state_global_memory = [] #Stores every characterics which are global for a state.\n",
        "        self.new_state_cars_memory = [] \n",
        "        self.new_state_global_memory = []\n",
        "        self.new_state_jobs_memory = []\n",
        "        self.action_memory = [] #Consider action going from 0 to nb_car // Size : self.mem_size*nb_actions_to_make_in_state(it varies)\n",
        "        self.timelapse_memory = []\n",
        "\n",
        "        self.terminal_memory = [] #We could use arrays here\n",
        "        self.reward_memory = [] #We could use arrays here\n",
        "        self.device = device\n",
        "\n",
        "    def push(self, state_cars, state_global, action, reward, new_state_cars, new_state_global, new_state_jobs, timelapse, done):\n",
        "        \"\"\"\n",
        "        Add a new sample and replace oldest one if full\n",
        "        \"\"\"\n",
        "        self.state_cars_memory += [state_cars]\n",
        "        self.state_global_memory += [state_global]\n",
        "        self.new_state_cars_memory += [new_state_cars]\n",
        "        self.new_state_global_memory += [new_state_global]\n",
        "        self.action_memory += [action]\n",
        "        self.reward_memory += [reward]\n",
        "        self.new_state_jobs_memory += [new_state_jobs]\n",
        "        self.timelapse_memory += [timelapse]\n",
        "        self.terminal_memory += [done]\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "        # Supress 1st element if too many of them.\n",
        "        if self.mem_cntr>self.max_size:\n",
        "          self.state_cars_memory.pop(0)\n",
        "          self.state_global_memory.pop(0)\n",
        "          self.new_state_cars_memory.pop(0)\n",
        "          self.new_state_global_memory.pop(0)\n",
        "          self.action_memory.pop(0)\n",
        "          self.reward_memory.pop(0)\n",
        "          self.new_state_jobs_memory.pop(0)\n",
        "          self.timelapse_memory.pop(0)\n",
        "          self.terminal_memory.pop(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample from the memory\n",
        "        return : list of size 'batch_size' containing different observations.\n",
        "        \"\"\"\n",
        "        max_mem = min(self.mem_cntr, self.max_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False) \n",
        "\n",
        "        states_cars = []\n",
        "        states_global = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        new_states_cars = []\n",
        "        new_states_global = []\n",
        "        new_state_jobs = []\n",
        "        terminal = []\n",
        "        timelapse = []\n",
        "        for ele in batch:\n",
        "          states_cars += [self.state_cars_memory[ele]]\n",
        "          states_global += [self.state_global_memory[ele]]\n",
        "          actions += [self.action_memory[ele]]\n",
        "          rewards += [self.reward_memory[ele]]\n",
        "          new_states_cars += [self.new_state_cars_memory[ele]]\n",
        "          new_states_global += [self.new_state_global_memory[ele]]\n",
        "          new_state_jobs += [self.new_state_jobs_memory[ele]]\n",
        "          terminal += [self.terminal_memory[ele]]\n",
        "          timelapse += [self.timelapse_memory[ele]]\n",
        "\n",
        "        return states_cars, states_global, actions, self.to_torch(rewards), new_states_cars, new_states_global, new_state_jobs, self.to_torch(timelapse), self.to_torch(terminal)\n",
        "\n",
        "    def to_torch(self, x):\n",
        "        return torch.tensor(x).to(self.device)\n",
        "\n",
        "    def to_numpy(self, x):\n",
        "        return x.detach().cpu().numpy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(self.mem_cntr, self.max_size) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "WmPDryyPBTy9"
      },
      "outputs": [],
      "source": [
        "# Network\n",
        "\n",
        "class ReqN(nn.Module):\n",
        "    def __init__(self, cars_input_size, global_input_size, nb_car, hidden_size_1=10,hidden_size_2=500):\n",
        "        super().__init__()\n",
        "        self.cars_input = cars_input_size\n",
        "        self.global_input = global_input_size\n",
        "        self.nb_car = nb_car\n",
        "        self.linear_1 = nn.Linear(cars_input_size, hidden_size_1)\n",
        "        self.linear_2 = nn.Linear(hidden_size_1, hidden_size_1)\n",
        "        self.linear_3 = nn.Linear(hidden_size_1*nb_car+global_input_size, hidden_size_2)\n",
        "        self.linear_4 = nn.Linear(hidden_size_2, hidden_size_2)\n",
        "        self.linear_5 = nn.Linear(hidden_size_2, nb_car+1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, y): #x : batch_size*(Nb_car)*x_input  ; y : batch_size*(nb_caracteristics_global : y_input)\n",
        "        x = self.linear_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = torch.flatten(x,start_dim=1)\n",
        "        x = torch.cat((x,y),1)\n",
        "\n",
        "        x = self.linear_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear_4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        output = self.linear_5(x)\n",
        "        if output.shape[0]==1:\n",
        "          x = x.squeeze(0)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "6d5JZUsQBYus"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, \n",
        "                 n_actions,\n",
        "                 memory, \n",
        "                 eps, eps_decay,\n",
        "                 discount_rate, \n",
        "                 update_delay, \n",
        "                 device\n",
        "                 ):\n",
        "        \n",
        "        self.action_space = [i for i in range(n_actions)] #n_actions = nb of cars + 1 (it's not possible to refuse an action)\n",
        "        self.memory = memory \n",
        "        self.eps, self.eps_decay = eps, eps_decay\n",
        "        self.discount_rate = discount_rate #Should be equal to 1 at some point!!\n",
        "        self.update_delay = update_delay\n",
        "        self.counter = 0\n",
        "        self.device = device\n",
        "\n",
        "        self.requests_repository = pd.DataFrame(columns=['time','day']) #Easier to store these values into a dataframe / Allows to have information on the lasts requests.\n",
        "\n",
        "    def init_nets(self, Q_net, target_net, optimizer, batch_size):\n",
        "        \"\"\"\n",
        "        initialize online and targets\n",
        "        \"\"\"\n",
        "        self.Q_net = Q_net\n",
        "        self.target_net = target_net\n",
        "        self.optimizer = optimizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.cars_input = self.Q_net.cars_input \n",
        "        self.global_input = self.Q_net.global_input\n",
        "        self.nb_car = self.Q_net.nb_car\n",
        "\n",
        "        self.copy_weights()\n",
        "        self.counter += 1\n",
        "    \n",
        "    def to_torch(self, x):\n",
        "        return torch.tensor(x).to(self.device).float()\n",
        "\n",
        "    def to_numpy(self, x):\n",
        "        return x.detach().cpu().numpy()\n",
        "\n",
        "    def create_mask(self, jobs, state_tensor_cars, state_tensor_global, prob_max=0.5): \n",
        "      \"\"\"\n",
        "      Create a mask for the car that we cannot assign to a certain request and that for every request of a state.\n",
        "      return: a list of 0 and 1 where 1 means that we mask the outcome, size: nb_request*(nb_car+1)\n",
        "      \"\"\"\n",
        "      # Deal with cars, for which the distance is too high.\n",
        "      nb_request = state_tensor_cars.shape[0]\n",
        "      time = state_tensor_global[0][1]\n",
        "      mask = torch.zeros(nb_request,self.nb_car+1).to(self.device)\n",
        "      for i in range(nb_request):\n",
        "        request_time = state_tensor_global[i][2]\n",
        "        for j in range(self.nb_car):\n",
        "          mask[i][j] = (state_tensor_cars[i][j][3] < prob_max) #if not accesible then 1.\n",
        "\n",
        "      # Deal with cars which already have too many jobs + Deal with 444 cars, in case that the assignment is too far!\n",
        "      for car in (jobs['323']+jobs['234']):\n",
        "        for i in range(nb_request):\n",
        "          mask[i,car] = 1 \n",
        "      return mask\n",
        "\n",
        "    def train(self, states_cars, states_global, targets, mask, list_association):\n",
        "        \"\"\"\n",
        "        Train online net for 1 step\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        self.Q_net.train()\n",
        "        Q_values = self.Q_net(states_cars,states_global).to(self.device) #nb_reqs(in the whole batch) *nb_actions \n",
        "\n",
        "        # Masking\n",
        "        Q_values = (Q_values*mask).sum(-1)\n",
        "\n",
        "        # Aggregate\n",
        "        aggreg_link = self.aggregation_link(Q_values, list_association)\n",
        "        Q_values = torch.transpose(aggreg_link,0,1) @ Q_values #batch_size*nb_actions\n",
        "\n",
        "        # Computing loss\n",
        "        loss = (targets.detach() - Q_values).pow(2).mean()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Apply gradients\n",
        "        self.optimizer.step()\n",
        "        self.Q_net.eval()\n",
        "\n",
        "        # Gather loss for analysis\n",
        "        return loss\n",
        "\n",
        "    def copy_weights(self):\n",
        "        \"\"\"\n",
        "        Copy weights from online to target net\n",
        "        \"\"\"\n",
        "        self.target_net.load_state_dict(self.Q_net.state_dict())\n",
        "\n",
        "    def select_action(self, states_cars, states_global, mask): #Optimize it by taking the action : no assignement (in the beggining) very frequently.\n",
        "        \"\"\"\n",
        "        Select an action with eps greedy as well as dealing with the overlapping issue\n",
        "        \"\"\"\n",
        "        # Epsilon greedy\n",
        "\n",
        "        list_action = []\n",
        "        for i,(state_cars,state_global) in enumerate(zip(states_cars,states_global)):\n",
        "          rand = np.random.random()\n",
        "          if rand < self.eps: #If we choose randomly\n",
        "            action = np.random.choice(self.action_space) \n",
        "            while (action in list_action or mask[i][action]==1) and action != self.nb_car: #Continue until we find an action we can realize. \n",
        "              action = np.random.choice(self.action_space)\n",
        "            list_action += [action]\n",
        "          else: #If we take the max Q value\n",
        "            tensor_action = self.Q_net(state_cars.unsqueeze(0),state_global.unsqueeze(0)).to(self.device)\n",
        "            tensor_action += tensor_action*mask[i]*(-100000) #tensor_action.masked_fill_(mask[i],-np.inf) \n",
        "            action = self.to_numpy(torch.max(tensor_action,1)[1])[0]\n",
        "            # Deal with overlapping actions : 1st arrived 1st served.\n",
        "            k = 2\n",
        "            while (action.item() in list_action or mask[i,action]==1) and action.item() != self.nb_car: #Continue until we find an action we can realize.\n",
        "              action = self.to_numpy(torch.topk(tensor_action.squeeze(0),k)[1][k-1])\n",
        "              k += 1\n",
        "            list_action += [action]                            \n",
        "        return np.array(list_action) #To improve: we don't need to create a list\n",
        "    \n",
        "    def select_min_action(self, states_cars, mask):\n",
        "        \"\"\"\n",
        "        Select the action that maximises the near rewards (the closest car)\n",
        "        \"\"\"\n",
        "        # \n",
        "\n",
        "        list_action = []\n",
        "        for i,state_cars in enumerate(states_cars):\n",
        "          action_min = torch.argmin(state_cars[:,2]) \n",
        "          if mask[i][action_min] == 1 or action_min in list_action:\n",
        "            list_action += [self.nb_car]\n",
        "          else:\n",
        "            list_action += [action_min.detach().item()]\n",
        "        return np.array(list_action)\n",
        "          \n",
        "    def remember(self, *args):\n",
        "        \"\"\"\n",
        "        Update memory\n",
        "        args: state_cars, state_global, action, reward, new_state_cars, new_state_global, new_state_jobs, timelapses, done\n",
        "        \"\"\"\n",
        "\n",
        "        self.memory.push(*args)\n",
        "\n",
        "    def regroup_tensor(self, states_cars, states_global):\n",
        "      \"\"\"\n",
        "      Regroup tensors stored into a list into a unique tensor\n",
        "      return : a unique tensor composed of every input tensors with a list which associates the input with the output\n",
        "      \"\"\"\n",
        "      # Create an association list between the elements of this batch and compute the total amount of request in this batch.\n",
        "      n_element = 0\n",
        "      list_association = []\n",
        "\n",
        "      for i,(state_cars,state_global) in enumerate(zip(states_cars,states_global)):\n",
        "        n_element += len(state_cars)\n",
        "        for j in range(len(state_cars)):\n",
        "          list_association += [[i,j]]\n",
        "      \n",
        "      # Fill the tensors we're going to use for the batch\n",
        "      states_cars_ = torch.zeros(n_element,self.nb_car,self.cars_input).to(self.device)\n",
        "      states_global_ = torch.zeros(n_element,self.global_input).to(self.device)\n",
        "\n",
        "      count = 0\n",
        "      for i,(state_cars,state_global) in enumerate(zip(states_cars,states_global)):\n",
        "        for j in range(len(state_cars)):\n",
        "          states_cars_[count] = state_cars[j]\n",
        "          states_global_[count] = state_global[j]\n",
        "          count += 1\n",
        "\n",
        "      return states_cars_,states_global_,list_association\n",
        "\n",
        "    def requests_repository_update(self,state):\n",
        "      \"\"\"\n",
        "      Update the requests_repository with the current timestep.\n",
        "      \"\"\"\n",
        "      time = state['time']\n",
        "      day = state['dow']\n",
        "      for req_time in state['request_times']:\n",
        "        if req_time == time:\n",
        "          self.requests_repository = self.requests_repository.append({'time': time,'day': day},ignore_index=True)\n",
        "      self.requests_repository_filter(time,day)\n",
        "\n",
        "    def requests_repository_filter(self,time,day,lim = 900):\n",
        "      self.requests_repository = self.requests_repository[(self.requests_repository['time']>time-900) & (self.requests_repository['day']==day)]\n",
        "\n",
        "    def dict_to_network(self,state):\n",
        "      \"\"\"\n",
        "      Takes (in input) the state dict and transforms it into a tensor while selecting the right features.\n",
        "      \"\"\"\n",
        "      # Build tensors 'cars' and 'global'.\n",
        "      n = len(state['request_times'])\n",
        "      cars_input = torch.zeros(n,self.nb_car,self.cars_input).to(self.device)\n",
        "      global_input = torch.zeros(n,self.global_input).to(self.device)\n",
        "      for i in range(n):\n",
        "        for car in range(self.nb_car):\n",
        "          cars_input[i][car][0] = normalize_x(state['v_locs'][car][0])\n",
        "          cars_input[i][car][1] = normalize_y(state['v_locs'][car][1])\n",
        "          # Calculate the distance to the request.\n",
        "          first_job_coord = state['v_job_locs'][car][0] #We retrieve it even if it's not necessary\n",
        "          second_job_coord = state['v_job_locs'][car][1] \n",
        "          third_job_coord = state['v_job_locs'][car][2]\n",
        "          cars_input[i][car][2] = self.to_torch(distance_to_request(state['v_locs'][car], state['request_locs'][i][0], state['v_jobs'][car], state['time'], first_job_coord, second_job_coord, third_job_coord))\n",
        "          cars_input[i][car][3] = proba_request_moins_tmin(state['v_locs'][car], state['request_locs'][i][0], state['v_jobs'][car], state['time'], state['request_times'][i], first_job_coord)\n",
        "          # Should also add the 'real' distance between the car and the request.\n",
        "        global_input[i][0:5] = torch.Tensor(np.array([i==state['dow'] for i in range(5)])) #day / one_hot encode\n",
        "        global_input[i][5] = state['time'] #Projeté dans un espace latent le time? \n",
        "        global_input[i][6] = state['request_times'][i] \n",
        "        # Amount of available cars (right away)\n",
        "        jobs = triplets_jobs(state) #Could add it to the argument of the function\n",
        "        global_input[i][7] = len(jobs['044']) + len(jobs['104']) + len(jobs['444'])  \n",
        "        # Amount of requests\n",
        "        global_input[i][8] = len(state['request_times'])\n",
        "        # Amount of requests in the last 15 mins.\n",
        "        global_input[i][9] = len(self.requests_repository['time'])\n",
        "        # Requests coordinate\n",
        "        global_input[i][10] = normalize_x(state['request_locs'][i][0][0]) #Beggining x\n",
        "        global_input[i][11] = normalize_y(state['request_locs'][i][0][1]) #Beggining y\n",
        "        global_input[i][12] = normalize_x(state['request_locs'][i][1][0]) #Arrival x\n",
        "        global_input[i][13] = normalize_x(state['request_locs'][i][1][1]) #Arrival y\n",
        "        global_input[i][14] = dist_manhattan(state['request_locs'][i][0],state['request_locs'][i][1])#Distance of request\n",
        "        global_input[i][15] = duree_deplacement(state['request_locs'][i][0],state['request_locs'][i][1],state['time'])[0]#Approximate time of request\n",
        "\n",
        "      return cars_input,global_input\n",
        "\n",
        "    def action_to_tensor(self, actions):\n",
        "      \"\"\"\n",
        "      Transforms the list of actions of a batch into a single tensor.\n",
        "      \"\"\"\n",
        "      list_res = []\n",
        "      for list_action in actions:\n",
        "        for action in list_action:\n",
        "          list_res += [action]\n",
        "      return self.to_torch(np.array(list_res))\n",
        "    \n",
        "    def aggregation_link(self, Q_values, list_association):\n",
        "      \"\"\"\n",
        "      Create a matrix of size : nb_requests(in the batch) * batch_size, \n",
        "      \"\"\"\n",
        "      mask = torch.zeros(Q_values.shape[0],self.batch_size).to(self.device) #Nb_reqs * batch_size \n",
        "      count = 0\n",
        "      for j in range(self.batch_size):\n",
        "        temp = []\n",
        "        for pair in list_association:\n",
        "          if pair[0] == j:\n",
        "            temp += [count]\n",
        "            count += 1\n",
        "        for i in temp:\n",
        "          mask[i][j] = 1/len(temp)\n",
        "      return mask\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Apply deep Q-learning algorithm step\n",
        "        \"\"\"\n",
        "        if len(self.memory) >= self.batch_size:\n",
        "\n",
        "            # Sample from memory\n",
        "            states_cars, states_global, actions, reward, new_states_cars, new_states_global, new_states_jobs, timelapses,done = self.memory.sample(self.batch_size) \n",
        "\n",
        "            # Compute the mask(action) on every element of the batch for new_state       \n",
        "            actions_new_state = []\n",
        "            for ele_cars,ele_global,ele_jobs in zip(new_states_cars,new_states_global,new_states_jobs):\n",
        "              mask = self.create_mask(ele_jobs,ele_cars,ele_global)\n",
        "              actions_new_state += [self.select_action(ele_cars,ele_global,mask)] #SARSA\n",
        "            actions_new_state = self.action_to_tensor(actions_new_state)\n",
        "            mask = F.one_hot(actions_new_state.long(),num_classes=self.nb_car+1)\n",
        "\n",
        "            # Regroup the states and compute the association lists\n",
        "            states_cars, states_global, list_association = self.regroup_tensor(states_cars, states_global)\n",
        "            new_states_cars, new_states_global, new_list_association = self.regroup_tensor(new_states_cars, new_states_global)\n",
        "\n",
        "            # Compute target Q value\n",
        "            target_Q_value = self.target_net(new_states_cars,new_states_global)\n",
        "\n",
        "            # We 'add' the mask\n",
        "            target_Q_value = (target_Q_value*mask).sum(-1)\n",
        "\n",
        "            # Agglomerate the different Q_value\n",
        "            aggreg_link = self.aggregation_link(target_Q_value,new_list_association)\n",
        "            Agglomerate_Q_value = torch.transpose(aggreg_link,0,1) @ target_Q_value \n",
        "\n",
        "            # Compute target\n",
        "            target_Q_value = reward.to(self.device) + torch.pow(self.discount_rate*torch.ones(self.batch_size).to(self.device),timelapses.to(self.device)/60).to(self.device)*Agglomerate_Q_value*done.to(self.device)  \n",
        "\n",
        "            # Change actions into usable tensors\n",
        "            actions = self.action_to_tensor(actions)\n",
        "\n",
        "            # Compute mask for actions\n",
        "            mask = F.one_hot(actions.long(),num_classes=self.nb_car+1)\n",
        "\n",
        "            # Train network         \n",
        "            loss = self.train(states_cars, states_global, target_Q_value, mask, list_association)\n",
        "\n",
        "            # Copy weights\n",
        "            if self.counter % self.update_delay == 0:\n",
        "                self.copy_weights()\n",
        "\n",
        "            # Update epsilon\n",
        "            self.eps *= self.eps_decay\n",
        "\n",
        "            self.counter += 1\n",
        "\n",
        "            # Return loss for analysis\n",
        "            return loss\n",
        "        else:\n",
        "            return\n",
        "\n",
        "# Save our models   \n",
        "\n",
        "    def save_model(self,episode,name):\n",
        "      \"\"\"\n",
        "      Save our models\n",
        "      \"\"\"\n",
        "      torch.save({\n",
        "          \"episode\":episode,\n",
        "          \"Q_net\":self.Q_net.state_dict(),\n",
        "          \"Optimizer\":self.optimizer.state_dict()\n",
        "      },name)\n",
        "    \n",
        "    def load_model(self,file):\n",
        "      \"\"\"\n",
        "      Load our models\n",
        "      \"\"\"\n",
        "      checkpoint = torch.load(file,map_location=self.device)\n",
        "      self.Q_net.load_state_dict(checkpoint['Q_net'])\n",
        "      self.target_net.load_state_dict(checkpoint['Q_net'])\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepQ Distil"
      ],
      "metadata": {
        "id": "V-zJSc1l3_sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DeepQ Distil Idea :**\n",
        "\n",
        "Our Network has too many variables to consider and to optimize, the different coordinates, as an example, give only a really small part of the information.\n",
        "We mentionned ealier 2 different ways of optimizing the selection process.\n",
        "* Select the right car\n",
        "* Select if a request should be done\n",
        "\n",
        "We consider that we can realize the first problem fast optimaly with an intuitiv heuristic. Which is why, we are only going to consider 2 output this time: either we take or we don't take the request. And if we take it, we'll assign the request depending on the heuristic we've implemented. (This Heuristic is here : We take the car which is the most probable to get the request, it could be improve but it's already a solid heuristic).\n",
        "\n",
        "In addition to that, we tried to reduce the variables we use to only use the variables which give a lot of information for the $2^{nd}$ problem. They are 11 of them that we've chosen: \n",
        "* Time\n",
        "* Request_time\n",
        "* Nb_available_car (%)\n",
        "* nb_requests in this epoch\n",
        "* nb_requests_last_15_mins\n",
        "* distance of the request\n",
        "* duration of the request (estimated)\n",
        "* Max probability among all cars that they can take this request.\n",
        "* Distance to request from car that has the biggest prob to get the request.\n",
        "* Car that has the biggest prob to get the request.\n",
        "* The probablilty\n",
        "\n",
        "\n",
        "\n",
        "In practice, there are a few changes : we've defined a child class (Agent_distil) of Agent because many functions are the same as in the previous network.\n",
        "We've reimplemented the functions that needed to be reimplemented in the Agent_distil class order to accept the new inputs and outputs. \n",
        "\n",
        "We've also defined a different memory and a different RQN class : ReplayBuffer2/ReqN2 with adapted input and return.\n",
        "\n",
        "There's one more thing, our network doesn't depend on the amount of cars in the environment anymore! Which means that we could apply the same network, pre_trained on the small dataset, on the big dataset."
      ],
      "metadata": {
        "id": "AIQKUFht6yx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory\n",
        "\n",
        "class ReplayBuffer2():\n",
        "    def __init__(self, max_size, device):\n",
        "        self.max_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "\n",
        "        self.state_global_memory = [] #Stores every characterics which are globals for a state.\n",
        "        self.new_state_global_memory = []\n",
        "        self.new_state_jobs_memory = []\n",
        "        self.action_memory = [] #Consider action going from 0 to nb_car // Size : self.mem_size*nb_actions_to_make_in_state(it varies)\n",
        "        self.timelapse_memory = []\n",
        "\n",
        "        self.terminal_memory = [] #We could use arrays here\n",
        "        self.reward_memory = [] #We could use arrays here\n",
        "        self.device = device\n",
        "\n",
        "    def push(self, state_global, action, reward, new_state_global, new_state_jobs, timelapse, done):\n",
        "        \"\"\"\n",
        "        Add a new sample and replace oldest one if full\n",
        "        \"\"\"\n",
        "        self.state_global_memory += [state_global]\n",
        "        self.new_state_global_memory += [new_state_global]\n",
        "        self.action_memory += [action]\n",
        "        self.reward_memory += [reward]\n",
        "        self.new_state_jobs_memory += [new_state_jobs]\n",
        "        self.timelapse_memory += [timelapse]\n",
        "        self.terminal_memory += [done]\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "        # Supress 1st element if too many of them.\n",
        "        if self.mem_cntr>self.max_size:\n",
        "          self.state_global_memory.pop(0)\n",
        "          self.new_state_global_memory.pop(0)\n",
        "          self.action_memory.pop(0)\n",
        "          self.reward_memory.pop(0)\n",
        "          self.new_state_jobs_memory.pop(0)\n",
        "          self.timelapse_memory.pop(0)\n",
        "          self.terminal_memory.pop(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample from the memory\n",
        "        return : list of size 'batch_size' containing different observations.\n",
        "        \"\"\"\n",
        "        max_mem = min(self.mem_cntr, self.max_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False) \n",
        "\n",
        "        states_global = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        new_states_global = []\n",
        "        new_state_jobs = []\n",
        "        terminal = []\n",
        "        timelapse = []\n",
        "        for ele in batch:\n",
        "          states_global += [self.state_global_memory[ele]]\n",
        "          actions += [self.action_memory[ele]]\n",
        "          rewards += [self.reward_memory[ele]]\n",
        "          new_states_global += [self.new_state_global_memory[ele]]\n",
        "          new_state_jobs += [self.new_state_jobs_memory[ele]]\n",
        "          terminal += [self.terminal_memory[ele]]\n",
        "          timelapse += [self.timelapse_memory[ele]]\n",
        "\n",
        "        return  states_global, actions, self.to_torch(rewards), new_states_global, new_state_jobs, self.to_torch(timelapse), self.to_torch(terminal)\n",
        "\n",
        "    def to_torch(self, x):\n",
        "        return torch.tensor(x).to(self.device)\n",
        "\n",
        "    def to_numpy(self, x):\n",
        "        return x.detach().cpu().numpy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(self.mem_cntr, self.max_size) "
      ],
      "metadata": {
        "id": "QbNvI1EUi2FX"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network\n",
        "\n",
        "class ReqN2(nn.Module):\n",
        "    def __init__(self, global_input_size, nb_car, hidden_size_1=30,hidden_size_2=200):\n",
        "        super().__init__()\n",
        "        self.global_input = global_input_size\n",
        "        self.nb_car = nb_car\n",
        "        self.linear_1 = nn.Linear(global_input_size, hidden_size_1)\n",
        "        self.linear_2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.linear_3 = nn.Linear(hidden_size_2, hidden_size_2)\n",
        "        self.linear_4 = nn.Linear(hidden_size_2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear_3(x)\n",
        "        x = self.relu(x)\n",
        "        output = self.linear_4(x)\n",
        "\n",
        "        if output.shape[0]==1: #If there's a useless first dimension (batch_size = 1 when we call 1 example)\n",
        "          x = x.squeeze(0)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "3NxqfLVviBsP"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent_distil(Agent):\n",
        "  def __init__(self, \n",
        "                 n_actions,\n",
        "                 memory, \n",
        "                 eps, eps_decay,\n",
        "                 discount_rate, \n",
        "                 update_delay, \n",
        "                 device\n",
        "                 ):\n",
        "    super().__init__(n_actions, #n_actions = 2 either wait or assign.\n",
        "                 memory, \n",
        "                 eps, eps_decay,\n",
        "                 discount_rate, \n",
        "                 update_delay, \n",
        "                 device)\n",
        "    \n",
        "  def init_nets_distil(self, Q_net, target_net, optimizer, batch_size):\n",
        "    \"\"\"\n",
        "    initialize online and targets\n",
        "    \"\"\"\n",
        "    self.Q_net = Q_net\n",
        "    self.target_net = target_net\n",
        "    self.optimizer = optimizer\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.global_input = self.Q_net.global_input\n",
        "    self.nb_car = self.Q_net.nb_car\n",
        "\n",
        "    self.copy_weights()\n",
        "    self.counter += 1\n",
        "\n",
        "  def select_action_distil(self,states_global):\n",
        "    # Select if we want to wait or assign + assign the car which has the most chance of getting this request. (could be improved)\n",
        "    list_action = []\n",
        "    for i,state_global in enumerate(states_global):\n",
        "      rand = np.random.random()\n",
        "      if rand < self.eps: #If we choose randomly\n",
        "        action = np.random.choice(self.action_space) #If a car would be assigned to 2 requests, it could pose a problem BUT really not important.\n",
        "        list_action += [action]\n",
        "      else:\n",
        "        tensor_action = self.Q_net(state_global.unsqueeze(0)).to(self.device)\n",
        "        action = self.to_numpy(torch.max(tensor_action,1)[1])[0]\n",
        "        list_action += [action]\n",
        "\n",
        "    return list_action\n",
        "\n",
        "  def action_to_car(self,states_global,list_action):\n",
        "    \"\"\"\n",
        "    Takes in input the states_global tensor and the action chosen for every request of a state and returns which car should be assigned to which request.\n",
        "    \"\"\"\n",
        "    list_res = []\n",
        "    for i,action in enumerate(list_action):\n",
        "      if action == 0:\n",
        "        car = int(states_global[i][9].item())\n",
        "        print('Car :',car)\n",
        "        print('Associated probability :',states_global[i][10])\n",
        "        if car not in list_res: #If car not already assigned, if so, then we will wait until the next epoch to assign the request : could be improve.\n",
        "          list_res += [car]\n",
        "        else:\n",
        "          list_res += [self.nb_car]\n",
        "      else:\n",
        "        list_res += [self.nb_car]\n",
        "    return np.array(list_res)\n",
        "\n",
        "  def regroup_tensor_distil(self, states_global):\n",
        "    \"\"\"\n",
        "    Regroup tensors stored into a list into a unique tensor\n",
        "    return : a unique tensor composed of every input tensors with a list which associates the input with the output\n",
        "    \"\"\"\n",
        "    # Create an association list between the elements of this batch and compute the total amount of request in this batch.\n",
        "    n_element = 0\n",
        "    list_association = []\n",
        "\n",
        "    for i,state_global in enumerate(states_global):\n",
        "      n_element += len(state_global)\n",
        "      for j in range(len(state_global)):\n",
        "        list_association += [[i,j]]\n",
        "    \n",
        "    # Fill the tensors we're going to use for the batch\n",
        "    states_global_ = torch.zeros(n_element,self.global_input).to(self.device)\n",
        "\n",
        "    count = 0\n",
        "    for i,state_global in enumerate(states_global):\n",
        "        for j in range(len(state_global)):\n",
        "          states_global_[count] = state_global[j]\n",
        "          count += 1\n",
        "\n",
        "    return states_global_,list_association\n",
        "\n",
        "  def dict_to_network_distil(self,state):\n",
        "    \"\"\"\n",
        "    Takes (in input) the state dict and transforms it into a tensor while selecting the right features.\n",
        "    \"\"\"\n",
        "    # Build tensors 'cars' and 'global'.\n",
        "    n = len(state['request_times'])\n",
        "    global_input = torch.zeros(n,self.global_input).to(self.device)\n",
        "    for i in range(n):\n",
        "      global_input[i][0] = state['time']/86400 #Projeté dans un espace latent le time? \n",
        "      global_input[i][1] = state['request_times'][i]/86400\n",
        "      # Amount of available cars (right away)\n",
        "      jobs = triplets_jobs(state) #Could add it to the argument of the function\n",
        "      global_input[i][2] = len(jobs['044']) + len(jobs['104']) + len(jobs['444'])  \n",
        "      # Amount of requests\n",
        "      global_input[i][3] = len(state['request_times'])\n",
        "      # Amount of requests in the last 15 mins.\n",
        "      global_input[i][4] = len(self.requests_repository['time'])\n",
        "      # Request infos\n",
        "      global_input[i][5] = dist_manhattan(state['request_locs'][i][0],state['request_locs'][i][1])#Distance of request\n",
        "      global_input[i][6] = duree_deplacement(state['request_locs'][i][0],state['request_locs'][i][1],state['time'])[0]#Approximate time of request\n",
        "      # Select the car which should receive the assignation. (to consider multiple cars would be a plus)\n",
        "      max_prob = 0\n",
        "      max_car = 0\n",
        "      for car in range(self.nb_car):\n",
        "        first_job_coord = state['v_job_locs'][car][0]\n",
        "        prob = proba_request_moins_tmin(state['v_locs'][car], state['request_locs'][i][0], state['v_jobs'][car], state['time'], state['request_times'][i], first_job_coord)\n",
        "        if prob>max_prob:\n",
        "          max_prob = prob\n",
        "          max_car = car\n",
        "      car = max_car\n",
        "      second_job_coord = state['v_job_locs'][car][1] # in the case that no car are available \n",
        "      third_job_coord = state['v_job_locs'][car][2]\n",
        "      global_input[i][9] = car\n",
        "      global_input[i][7] = self.to_torch(true_distance_to_request(state['v_locs'][car], state['request_locs'][i][0], state['v_jobs'][car], first_job_coord, second_job_coord, third_job_coord))\n",
        "      global_input[i][8] = self.to_torch(distance_to_request(state['v_locs'][car], state['request_locs'][i][0], state['v_jobs'][car], state['time'], first_job_coord, second_job_coord, third_job_coord))\n",
        "      global_input[i][10] = max_prob\n",
        "    return global_input\n",
        "\n",
        "  def step_distil(self):\n",
        "      \"\"\"\n",
        "      Apply deep Q-learning algorithm step\n",
        "      \"\"\"\n",
        "      if len(self.memory) >= self.batch_size:\n",
        "          # Sample from memory\n",
        "          states_global, actions, reward, new_states_global, new_states_jobs, timelapses,done = self.memory.sample(self.batch_size) \n",
        "\n",
        "          # Regroup the states and compute the association lists\n",
        "          states_global, list_association = self.regroup_tensor_distil(states_global)\n",
        "          new_states_global, new_list_association = self.regroup_tensor_distil(new_states_global)\n",
        "\n",
        "          # Compute target Q value\n",
        "          target_Q_value = self.target_net(new_states_global)\n",
        "          target_Q_value = torch.max(target_Q_value,1)[0]\n",
        "\n",
        "          # Agglomerate the different Q_value\n",
        "          aggreg_link = self.aggregation_link(target_Q_value,new_list_association)\n",
        "          Agglomerate_Q_value = torch.transpose(aggreg_link,0,1) @ target_Q_value \n",
        "\n",
        "          # Compute target\n",
        "          target_Q_value = reward + torch.pow(self.discount_rate*torch.ones(self.batch_size).to(self.device),timelapses.to(self.device)/60).to(self.device)*Agglomerate_Q_value*done.to(self.device)  \n",
        "\n",
        "          # Change actions into usable tensors\n",
        "          actions = self.action_to_tensor(actions)\n",
        "\n",
        "          # Compute mask for actions\n",
        "          mask = F.one_hot(actions.long(),num_classes=2)\n",
        "\n",
        "          # Train network         \n",
        "          loss = self.train_distil(states_global, target_Q_value, mask, list_association)\n",
        "\n",
        "          # Copy weights\n",
        "          if self.counter % self.update_delay == 0:\n",
        "              self.copy_weights()\n",
        "\n",
        "          # Update epsilon\n",
        "          self.eps *= self.eps_decay\n",
        "\n",
        "          self.counter += 1\n",
        "\n",
        "          # Return loss for analysis\n",
        "          return loss\n",
        "      else:\n",
        "          return\n",
        "\n",
        "  def train_distil(self, states_global, targets, mask, list_association):\n",
        "      \"\"\"\n",
        "      Train online net for 1 step\n",
        "      \"\"\"\n",
        "      self.optimizer.zero_grad()\n",
        "      # Forward pass\n",
        "      self.Q_net.train()\n",
        "      Q_values = self.Q_net(states_global).to(self.device) #nb_reqs(in the whole batch) *nb_actions \n",
        "\n",
        "      # Masking\n",
        "      Q_values = (Q_values*mask).sum(-1)\n",
        "\n",
        "      # Aggregate\n",
        "      aggreg_link = self.aggregation_link(Q_values, list_association)\n",
        "      Q_values = torch.transpose(aggreg_link,0,1) @ Q_values #batch_size*nb_actions\n",
        "\n",
        "      # Computing loss\n",
        "      loss = (targets.detach() - Q_values).pow(2).mean()\n",
        "      loss.backward()\n",
        "        \n",
        "      # Apply gradients\n",
        "      self.optimizer.step()\n",
        "      self.Q_net.eval()\n",
        "\n",
        "      # Gather loss for analysis\n",
        "      return loss\n",
        "  "
      ],
      "metadata": {
        "id": "DQYEpZPH2pEr"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantage"
      ],
      "metadata": {
        "id": "SaGVNnBZHe9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantage explanation**\n",
        "\n",
        "The idea here is to make sure that the Q_values estimate a value which is interpretable and with which we could optimally reduce the loss to 0.\n",
        "\n",
        "There aren't many differences with the classic algorithm, only that we split the approximation of Q(state,action) into Q(state) and Q(action in this state).\n",
        "Classicaly, we estimate Q(state) with a network when using advantage, but the last iterations give us a lot of informations on the Q(state). \n",
        "For every last 10 steps, for example, we have information on the cumulative reward at many timeframes.\n",
        "We know that, as an example, the expected cumulative reward was at t=0 : 15000 (aggregation of last values). And at t=10000, 14000. \n",
        "We are then going to use these estimation to get Q(state).\n",
        "\n",
        "That's the general idea."
      ],
      "metadata": {
        "id": "_S8UKMFw0BF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonctions de l'avantage\n",
        "# We need to debug this function and then incorporate into the loss the \"advantage\" value, it should be quite short to have!\n",
        "\n",
        "def infer_rewards(reward_list, timestep_list, fixed_timesteps, infered_rewards, mean_score):            \n",
        "  infered_rewards[0].append(mean_score) # reward for time=0\n",
        "  indice = len(infered_rewards[0]) - 1\n",
        "  counter = 0\n",
        "  for j in range(1,len(fixed_timesteps)-1):\n",
        "    start_value = fixed_timesteps[j-1]\n",
        "    threshold = fixed_timesteps[j]\n",
        "    reward_sum = 0\n",
        "    \n",
        "    while timestep_list[counter][indice] <= threshold:\n",
        "      if timestep_list[counter][indice] <= start_value and timestep_list[counter][indice+1] <= threshold:  # if the epoch starts at the previous timestep and ends in the current one\n",
        "        truncated_interval = timestep_list[counter][indice+1] - start_value\n",
        "        true_interval = timestep_list[counter][indice+1] - timestep_list[counter][indice]\n",
        "        coef = truncated_interval / true_interval\n",
        "        reward_sum += reward_list[counter][indice] * coef\n",
        "      elif timestep_list[counter][indice] <= start_value and timestep_list[counter][indice+1] > threshold:  # if the epoch starts at the previous timestep and ends in the next one\n",
        "        truncated_interval = threshold - start_value\n",
        "        true_interval = timestep_list[counter][indice+1] - timestep_list[counter][indice]\n",
        "        coef = truncated_interval / true_interval\n",
        "        reward_sum += reward_list[counter][indice] * coef\n",
        "      elif timestep_list[counter][indice+1] <= threshold:  # if the epoch starts and ends in the current timestep\n",
        "        reward_sum += reward_list[counter][indice]\n",
        "      elif timestep_list[counter][indice+1] > threshold:  # if the epoch starts in the current timestep and ends in the next one\n",
        "        truncated_interval = threshold - timestep_list[counter][indice]\n",
        "        true_interval = timestep_list[counter][indice+1] - timestep_list[counter][indice]\n",
        "        coef = truncated_interval / true_interval\n",
        "        reward_sum += reward_list[counter][indice] * coef\n",
        "      counter += 1\n",
        "\n",
        "    infered_rewards[j].append(reward_sum)\n",
        "\n",
        "  infered_rewards[-1].append(0)  # reward for time=86400\n",
        "  return infered_rewards\n",
        "\n",
        "\n",
        "def state_value(time,mean_score,mean_rewards,fixed_timesteps):\n",
        "  counter = 1\n",
        "  while fixed_timesteps[counter] < time:  # put the threshold at the end of the running timestep\n",
        "    counter += 1\n",
        "\n",
        "  coef = (time - fixed_timesteps[counter-1]) / (fixed_timesteps[counter] - fixed_timesteps[counter-1])\n",
        "  partial_reward = mean_rewards[counter-1] * coef\n",
        "  value = mean_score\n",
        "\n",
        "  for i in range(counter-1):  # compute what is left to expect as a reward\n",
        "    value -= mean_rewards[i]\n",
        "  value -= partial_reward\n",
        "  return value"
      ],
      "metadata": {
        "id": "bJEoy9PoHkmf"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main 1"
      ],
      "metadata": {
        "id": "tmwgL2tg4euv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6a7rW79NdLqT",
        "outputId": "b58bf3aa-2f95-473e-e075-7c71fd8e9640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/101 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([7])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 625.0345259884056\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 865.21927673465\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14,  0])), ('req_rejections', array([0, 0]))])\n",
            "day : 1/ time : 884.042432787471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Ignoring infeasible assignments for the following vehicles:\n",
            "Int64Index([5], dtype='int64').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14,  5])), ('req_rejections', array([0, 0]))])\n",
            "day : 1/ time : 890.0368569963732\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14,  3])), ('req_rejections', array([0, 0]))])\n",
            "day : 1/ time : 950.0368569963732\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1010.0368569963732\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 198, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1029.7153037799544\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1089.7153037799544\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1149.7153037799544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Ignoring infeasible assignments for the following vehicles:\n",
            "Int64Index([0], dtype='int64').\n",
            "WARNING:root:Ignoring infeasible assignments for the following vehicles:\n",
            "Int64Index([0], dtype='int64').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([0])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1717.5462573763089\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([0])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1777.5462573763089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Ignoring infeasible assignments for the following vehicles:\n",
            "Int64Index([0], dtype='int64').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([0])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1837.5462573763089\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1897.5462573763089\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 1957.5462573763089\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 1/ time : 2017.5462573763089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/101 [00:07<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-d3bb51068755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m                   \u001b[0maction_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheuristic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'reposition'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'req_assgts'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'req_rejections'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyhailing/ridehail_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1143\u001b[0m         rel_elapsed_ts = (\n\u001b[1;32m   1144\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mnext_epoch_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vehicles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprogress_vs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"j1ot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m             \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vehicles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprogress_vs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"j1dt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vehicles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprogress_vs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"j1ot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         )\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_elapsed_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"All first jobs should have finite elapsed times (non-zero duration).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;31m# This is an elided recursive call to iloc/loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not applicable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_bool_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_bool_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main1 = True\n",
        "if main1:\n",
        "  # Training Loop\n",
        "  MEMORY_SIZE = 10000\n",
        "  CARS_INPUT_SIZE = 4\n",
        "  GLOBAL_INPUT_SIZE = 16\n",
        "  N_CAR = env.num_vehicles\n",
        "\n",
        "  LR = 0.001\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "  N_SIMULATION = 101\n",
        "  EPS = 0.9\n",
        "  EPS_DECAY = 0.9995\n",
        "  DISCOUNT_RATE = 0.99 # should depend of t\n",
        "  UPDATE_DELAY = 50 # delay between target_net parameters updates\n",
        "  DEVICE = \"cuda\" # \"cuda\" or \"cpu\"\n",
        "\n",
        "  # Model and target model \n",
        "  Q_net = ReqN(CARS_INPUT_SIZE, GLOBAL_INPUT_SIZE, N_CAR).to(DEVICE)\n",
        "  target_net = ReqN(CARS_INPUT_SIZE, GLOBAL_INPUT_SIZE, N_CAR).to(DEVICE)\n",
        "\n",
        "  # Optimizer (only on Q_net)\n",
        "  optimizer = torch.optim.Adam(Q_net.parameters(), lr=LR)\n",
        "\n",
        "  # Memory\n",
        "  memory = ReplayBuffer(MEMORY_SIZE, DEVICE)\n",
        "\n",
        "  # Agent and initialization\n",
        "  agent = Agent(n_actions=N_CAR+1, \n",
        "                memory=memory, \n",
        "                eps=EPS, \n",
        "                eps_decay=EPS_DECAY,\n",
        "                discount_rate=DISCOUNT_RATE,\n",
        "                update_delay=UPDATE_DELAY, \n",
        "                device=DEVICE\n",
        "                )\n",
        "\n",
        "  agent.init_nets(Q_net, target_net, optimizer, BATCH_SIZE)\n",
        "  # Load a model\n",
        "  #agent.load_model('episode_20')\n",
        "\n",
        "  all_scores = []\n",
        "  all_loss = []\n",
        "  # Progress bar\n",
        "  with tqdm.tqdm(total=N_SIMULATION, position=0, leave=True) as pbar:\n",
        "      for i in range(N_SIMULATION):\n",
        "          done = False\n",
        "          score = 0\n",
        "          loss_episode_i = []\n",
        "          # Reset env\n",
        "          state = env.reset()\n",
        "\n",
        "          list_wait = [] # initialize the wait list (car which can't be assign at t but will be at t+eps(t))\n",
        "          \n",
        "          # Initialiaze the best model:\n",
        "          best_model = agent.Q_net.state_dict()\n",
        "          best_reward = 0\n",
        "\n",
        "          # Make sure that the first state is a state with request\n",
        "          while len(state['request_times']) == 0:\n",
        "            action_rep = heuristic(state,list_wait)\n",
        "            action = {'reposition': action_rep, 'req_assgts': np.array([]), 'req_rejections': np.array([])}\n",
        "            state, reward, _, _ = env.step(action)\n",
        "            score += reward\n",
        "\n",
        "          #Set the heuristic counter\n",
        "          counter_15_min = 100000#-901\n",
        "          counter_3_min = 100000#-181\n",
        "\n",
        "          #Store the states and other useful informations in a list for the analysis.\n",
        "          list_state = []\n",
        "          list_state += [state]\n",
        "          state_tensor_cars,state_tensor_global = agent.dict_to_network(state)\n",
        "          nb_available_car = []\n",
        "          actual_time = 900\n",
        "          amount_new_request_15_min = []\n",
        "    \n",
        "          # Update request repositories\n",
        "          agent.requests_repository_update(state)\n",
        "\n",
        "          # Count the iteration of the while loop\n",
        "          count_while = 0\n",
        "\n",
        "          while not done:\n",
        "              print('Running...')\n",
        "              # Retrieve lists of triplets from state\n",
        "              jobs = triplets_jobs(state)\n",
        "\n",
        "              # Update \"amount_new_request_15_min\" repository\n",
        "              if state['time'] > actual_time:\n",
        "                amount_new_request_15_min += [[state['time'],len(agent.requests_repository)]]\n",
        "                actual_time += 900\n",
        "\n",
        "              # Create the mask for every request (taking into account the distance)\n",
        "              mask = agent.create_mask(jobs,state_tensor_cars,state_tensor_global)\n",
        "\n",
        "              # Select action\n",
        "              action_req = agent.select_action(state_tensor_cars,state_tensor_global, mask) #agent.select_min_action(state_tensor_cars, mask)  \n",
        "              \n",
        "              # Change the state for the heuristic only\n",
        "              state_for_heuristic = update_state(state,action_req)\n",
        "\n",
        "              # Apply Heuristic # Should store the repos results we get from the optimized heuristics for the 444 cars.\n",
        "              if state['time']>900+counter_15_min:\n",
        "                action_rep,list_wait = heuristique_optimized(state_for_heuristic,delta_t=900)\n",
        "                counter_15_min = state['time']\n",
        "                counter_3_min = state['time']\n",
        "              elif state['time']>180+counter_3_min:\n",
        "                action_rep,list_wait = heuristique_optimized(state_for_heuristic,delta_t=180)\n",
        "                counter_3_min = state['time']\n",
        "              else:\n",
        "                action_rep = heuristic(state_for_heuristic,list_wait)\n",
        "\n",
        "              # Construct action # A not so probable error to correct : if request assign at the same time that a reposition is requested : gotta change the status!!!!!!!\n",
        "              action = OrderedDict({'reposition': action_rep, 'req_assgts': action_req, 'req_rejections': np.zeros_like(action_req)}) \n",
        "\n",
        "              # Print the action realised during this round\n",
        "              print('Action:', action)\n",
        "              day = state['dow']\n",
        "              time = state['time']\n",
        "              print(f'day : {day}/ time : {time}')\n",
        "\n",
        "              # Execute action\n",
        "              new_state, reward, done, _ = env.step(action) #Should compute the mean reward.\n",
        "                \n",
        "              # While no request for new_state : Apply heuristic and create a new action.\n",
        "              while len(new_state['request_times']) == 0 and not done:\n",
        "                if new_state['time']>900+counter_15_min:\n",
        "                  action_rep,list_wait = heuristique_optimized(new_state,delta_t=900)\n",
        "                  counter_15_min = new_state['time']\n",
        "                  counter_3_min = new_state['time']\n",
        "                elif new_state['time']>180+counter_3_min:\n",
        "                  action_rep,list_wait = heuristique_optimized(new_state,delta_t=180)\n",
        "                  counter_3_min = new_state['time']\n",
        "                else: \n",
        "                  action_rep = heuristic(new_state,list_wait)\n",
        "                action = {'reposition': action_rep, 'req_assgts': np.array([]), 'req_rejections': np.array([])}\n",
        "                new_state, reward_add, done, _ = env.step(action)\n",
        "                reward += reward_add\n",
        "                \n",
        "              score += reward\n",
        "              if not done: #to deal with the end of the environnment in the case that it happens during the while loop.\n",
        "                # Update request repositories\n",
        "                agent.requests_repository_update(new_state)\n",
        "\n",
        "                # Transforms state into a usable tensor for the : request network\n",
        "                new_state_tensor_cars,new_state_tensor_global = agent.dict_to_network(new_state)\n",
        "\n",
        "                # Update memory\n",
        "                new_jobs = triplets_jobs(new_state)\n",
        "                timelapse = new_state['time'] - state['time']\n",
        "                agent.remember(state_tensor_cars,state_tensor_global, action_req, reward, new_state_tensor_cars, new_state_tensor_global, new_jobs, timelapse,1-int(done))\n",
        "\n",
        "                # Apply algorithm and recuperate the loss for the analysis.\n",
        "                if count_while%30==0:\n",
        "                  loss = agent.step()\n",
        "                  if loss != None:\n",
        "                    loss_episode_i += [loss.detach().cpu().item()]\n",
        "\n",
        "                # Update count_while \n",
        "                count_while += 1\n",
        "\n",
        "                # Update state\n",
        "                state = new_state\n",
        "                state_tensor_cars,state_tensor_global = new_state_tensor_cars,new_state_tensor_global\n",
        "\n",
        "                # Update analysis lists\n",
        "                list_state += [state]\n",
        "                nb_available_car += [len(jobs['104'])+len(jobs['044'])+len(jobs['444'])]\n",
        "          \n",
        "          # Keep on with the model only if it's the best.\n",
        "          if score<best_reward:\n",
        "            agent.Q_net.load_state_dict(best_model)\n",
        "            agent.target_net.load_state_dict(best_model)\n",
        "          else:\n",
        "            best_reward = score\n",
        "            best_model = agent.Q_net.state_dict()\n",
        "\n",
        "          all_scores.append(score)\n",
        "          all_loss.append(np.mean(loss_episode_i))\n",
        "\n",
        "          pbar.set_description('score=' + str(score))\n",
        "          pbar.update()\n",
        "\n",
        "          # Store the results\n",
        "\n",
        "          if i%5==0:#Gather every 5 episodes\n",
        "            # Save model \n",
        "            name = 'episode_' + str(i) + 'deepQ_classical'\n",
        "            agent.save_model(i,name)\n",
        "\n",
        "            # Get results for the analysis\n",
        "            name_score = name + '-score'\n",
        "            np.save(name_score,all_scores)\n",
        "            name_loss = name + '-loss'\n",
        "            np.save(name_loss,all_loss)\n",
        "\n",
        "  plt.plot(all_scores)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main 2"
      ],
      "metadata": {
        "id": "vVYnm0T8jGxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "775a6363-57da-44c7-b26c-886b123adf33",
        "id": "SQq8Np9OjS7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/101 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running...\n",
            "Car : 4\n",
            "Associated probability : tensor(0.9999)\n",
            "[0.8384822739862043, [[9, 28.0], [11, 246.0], [13, 242.0], [7, 196.0], [3, 200.0], [12, 185.0], [1, 172.0], [4, 202.0], [2, 286.0], [0, 94.0], [6, 144.0], [8, 197.0], [10, 240.0], [5, 198.0]], 0.8480720514437379]\n",
            "Action: OrderedDict([('reposition', array([ 94, 172, 286, 200, 302, 198, 144, 302, 197,  28, 240, 246, 185,\n",
            "       242])), ('req_assgts', array([4])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 150.70818221554316\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 236.06548453602127\n",
            "Running...\n",
            "Car : 2\n",
            "Associated probability : tensor(0.9998)\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([2])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 296.06548453602124\n",
            "[0.7252498412380469, [[3, 200], [7, 196], [9, 28], [12, 185], [13, 242], [4, 245], [1, 172.0], [5, 171.0], [8, 202.0], [11, 246.0]], 0.7293072508088109]\n",
            "[0.7724644568921014, [[0, 94], [1, 172], [3, 200], [4, 245], [6, 144], [7, 196], [8, 197], [9, 28], [11, 246], [12, 185], [13, 242], [5, 195.0]], 0.7743091149996697]\n",
            "Running...\n",
            "Car : 13\n",
            "Associated probability : tensor(0.9861)\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([13])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 642.0188344930472\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 688.4967233538481\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 748.4967233538481\n",
            "Running...\n",
            "Car : 3\n",
            "Associated probability : tensor(0.9476)\n",
            "[0.7107410349427304, [[0, 94], [1, 242.0], [6, 144], [8, 197], [9, 28.0], [11, 202.0], [4, 235.0], [12, 297.0], [5, 164.0], [7, 196]], 0.7150574080729081]\n",
            "Action: OrderedDict([('reposition', array([302, 242, 302, 302, 235, 164, 302, 302, 302, 302, 302, 202, 297,\n",
            "       302])), ('req_assgts', array([3])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 808.4967233538481\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 821.1734437075191\n",
            "Running...\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([14])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 881.1734437075191\n",
            "Running...\n",
            "Car : 11\n",
            "Associated probability : tensor(0.5891)\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       285])), ('req_assgts', array([11])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 930.7781762478877\n",
            "[0.7116082060569183, [[0, 94], [1, 242], [4, 235], [5, 164], [6, 144], [7, 196], [8, 197], [9, 28], [12, 297], [13, 285]], 0.7116082060569183]\n",
            "Running...\n",
            "Car : 7\n",
            "Associated probability : tensor(0.8370)\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([7])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 1014.7671859118032\n",
            "[0.8230139143501063, [[1, 202.0], [5, 159.0], [12, 297.0], [6, 87.0], [8, 246.0], [4, 259.0], [3, 200.0], [9, 96.0], [13, 172.0], [0, 94], [10, 276.0], [2, 255.0]], 0.8325156999961284]\n",
            "Running...\n",
            "Car : 4\n",
            "Associated probability : tensor(0.9999)\n",
            "Action: OrderedDict([('reposition', array([302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,\n",
            "       302])), ('req_assgts', array([4])), ('req_rejections', array([0]))])\n",
            "day : 4/ time : 1080.1107675357136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/101 [00:35<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-1d6c118c4dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m                   \u001b[0mcounter_3_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                   \u001b[0maction_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheuristic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'reposition'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'req_assgts'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'req_rejections'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-0aac3f75f38f>\u001b[0m in \u001b[0;36mheuristic\u001b[0;34m(state, list_wait)\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mreposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'444'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcar_assign\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplus_proche_lot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_locs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'444'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mreposition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'444'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-df0ae346e790>\u001b[0m in \u001b[0;36mplus_proche_lot\u001b[0;34m(coords_voiture, time)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdurees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdurees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduree_deplacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords_voiture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdurees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-4a8ec2dc540a>\u001b[0m in \u001b[0;36mduree_deplacement\u001b[0;34m(list_coord_depart, list_coord_arrivee, time_)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mzone_arrivee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxy_to_zone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrivee\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtranche_horaire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mvitesse_moyenne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'puzone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mzone_depart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dozone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mzone_arrivee\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtranche_horaire\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'puzone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mzone_depart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dozone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mzone_arrivee\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspeeds_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtranche_horaire\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed_stddev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m#vitesse_associee = vitesse_normalisee(vitesse_moyenne, sigma) #WAY TO MUCH TIME TO PROCESS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5502\u001b[0m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_logical_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   2943\u001b[0m         \u001b[0;31m# We do not pass dtype to ensure that the Series constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m         \u001b[0;31m#  does inference in the case where `result` has object-dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2945\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         if (\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleArrayManager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "exec_distil = True\n",
        "if exec_distil:\n",
        "  # Training Loop\n",
        "\n",
        "  MEMORY_SIZE = 100000\n",
        "  GLOBAL_INPUT_SIZE = 11\n",
        "  N_CAR = env.num_vehicles\n",
        "\n",
        "  LR = 0.001\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "  N_SIMULATION = 101\n",
        "  EPS = 1\n",
        "  EPS_DECAY = 0.997\n",
        "  DISCOUNT_RATE = 0.99 # should depend of t\n",
        "  UPDATE_DELAY = 50 # delay between target_net parameters updates\n",
        "  DEVICE = \"cpu\" # \"cuda\" or \"cpu\"\n",
        "\n",
        "  # Model and target model \n",
        "  Q_net = ReqN2(GLOBAL_INPUT_SIZE, N_CAR).to(DEVICE)\n",
        "  target_net = ReqN2(GLOBAL_INPUT_SIZE, N_CAR).to(DEVICE)\n",
        "\n",
        "  # Optimizer (only on Q_net)\n",
        "  optimizer = torch.optim.Adam(Q_net.parameters(), lr=LR)\n",
        "\n",
        "  # Memory\n",
        "  memory = ReplayBuffer2(MEMORY_SIZE, DEVICE)\n",
        "\n",
        "  # Agent and initialization\n",
        "  agent = Agent_distil(n_actions=2, \n",
        "                memory=memory, \n",
        "                eps=EPS, \n",
        "                eps_decay=EPS_DECAY,\n",
        "                discount_rate=DISCOUNT_RATE,\n",
        "                update_delay=UPDATE_DELAY, \n",
        "                device=DEVICE\n",
        "                )\n",
        "\n",
        "  agent.init_nets_distil(Q_net, target_net, optimizer, BATCH_SIZE)\n",
        "  # Load a model\n",
        "  #agent.load_model('episode_25 DeepQ_notExact')\n",
        "\n",
        "  all_scores = []\n",
        "  all_loss = []\n",
        "  # Progress bar\n",
        "  with tqdm.tqdm(total=N_SIMULATION, position=0, leave=True) as pbar:\n",
        "      for i in range(N_SIMULATION):\n",
        "          done = False\n",
        "          score = 0\n",
        "          loss_episode_i = []\n",
        "          # Reset env\n",
        "          state = env.reset()\n",
        "\n",
        "          list_wait = [] # initialize the wait list (car which can't be assign at t but will be at t+eps(t))\n",
        "          \n",
        "          # Initialiaze the best model:\n",
        "          best_model = agent.Q_net.state_dict()\n",
        "          best_reward = 0\n",
        "\n",
        "          # Make sure that the first state is a state with request\n",
        "          while len(state['request_times']) == 0:\n",
        "            action_rep = heuristic(state,list_wait)\n",
        "            action = {'reposition': action_rep, 'req_assgts': np.array([]), 'req_rejections': np.array([])}\n",
        "            state, reward, _, _ = env.step(action)\n",
        "            score += reward\n",
        "\n",
        "          #Set the heuristic counter\n",
        "          counter_15_min = -901 #Initialize last time where heuristic was used by negativ values\n",
        "          counter_3_min = -181\n",
        "\n",
        "          #Store the states and other useful informations in a list for the analysis.\n",
        "          list_state = []\n",
        "          list_state += [state]\n",
        "          state_tensor_global = agent.dict_to_network_distil(state)\n",
        "          nb_available_car = []\n",
        "          actual_time = 900\n",
        "          amount_new_request_15_min = []\n",
        "    \n",
        "          # Update request repositories\n",
        "          agent.requests_repository_update(state)\n",
        "\n",
        "          # Count the iteration of the while loop\n",
        "          count_while = 0\n",
        "\n",
        "          while not done:\n",
        "              print('Running...')\n",
        "              # Retrieve lists of triplets from state\n",
        "              jobs = triplets_jobs(state)\n",
        "\n",
        "              # Update \"amount_new_request_15_min\" repository\n",
        "              if state['time'] > actual_time:\n",
        "                amount_new_request_15_min += [[state['time'],len(agent.requests_repository)]]\n",
        "                actual_time += 900\n",
        "\n",
        "              # Select action\n",
        "              action_req_binary = agent.select_action_distil(state_tensor_global)\n",
        "              action_req = agent.action_to_car(state_tensor_global,action_req_binary)\n",
        "\n",
        "              # Change the state for the heuristic only\n",
        "              state_for_heuristic = update_state(state,action_req)\n",
        "\n",
        "              # Apply Heuristic # Should store the repos results we get from the optimized heuristics for the 444 cars.\n",
        "              if state['time']>900+counter_15_min:\n",
        "                action_rep,list_wait = heuristique_optimized(state_for_heuristic,delta_t=900)\n",
        "                counter_15_min = state['time']\n",
        "                counter_3_min = state['time']\n",
        "              elif state['time']>180+counter_3_min:\n",
        "                action_rep,list_wait = heuristique_optimized(state_for_heuristic,delta_t=180)\n",
        "                counter_3_min = state['time']\n",
        "              else:\n",
        "                action_rep = heuristic(state_for_heuristic,list_wait)\n",
        "\n",
        "              # Construct action # A not so probable error to correct : if request assign at the same time that a reposition is requested : gotta change the status!!!!!!!\n",
        "              action = OrderedDict({'reposition': action_rep, 'req_assgts': action_req, 'req_rejections': np.zeros_like(action_req)}) #Need to deal with the rejections probably #May need to be an ordered dict\n",
        "\n",
        "              # Print the action realised during this round\n",
        "              print('Action:', action)\n",
        "              day = state['dow']\n",
        "              time = state['time']\n",
        "              print(f'day : {day}/ time : {time}')\n",
        "\n",
        "              # Execute action\n",
        "              new_state, reward, done, _ = env.step(action) #Should compute the mean reward.\n",
        "                \n",
        "              # While no request for new_state : Apply heuristic and create a new action.\n",
        "              while len(new_state['request_times']) == 0 and not done:\n",
        "                if new_state['time']>900+counter_15_min:\n",
        "                  action_rep,list_wait = heuristique_optimized(new_state,delta_t=900)\n",
        "                  counter_15_min = new_state['time']\n",
        "                  counter_3_min = new_state['time']\n",
        "                elif new_state['time']>180+counter_3_min:\n",
        "                  action_rep,list_wait = heuristique_optimized(new_state,delta_t=180)\n",
        "                  counter_3_min = new_state['time']\n",
        "                else: \n",
        "                  action_rep = heuristic(new_state,list_wait)\n",
        "                action = {'reposition': action_rep, 'req_assgts': np.array([]), 'req_rejections': np.array([])}\n",
        "                new_state, reward_add, done, _ = env.step(action)\n",
        "                reward += reward_add\n",
        "                \n",
        "              score += reward\n",
        "              if not done: #to deal with the end of the environnment in the case that it happens during the while loop.\n",
        "                # Update request repositories\n",
        "                agent.requests_repository_update(new_state)\n",
        "\n",
        "                # Transforms state into a usable tensor for the : request network\n",
        "                new_state_tensor_global = agent.dict_to_network_distil(new_state)\n",
        "\n",
        "                # Update memory\n",
        "                new_jobs = triplets_jobs(new_state)\n",
        "                timelapse = new_state['time'] - state['time']\n",
        "                agent.remember(state_tensor_global, action_req_binary, reward, new_state_tensor_global, new_jobs, timelapse,1-int(done))\n",
        "\n",
        "                # Apply algorithm and recuperate the loss for the analysis.\n",
        "                if count_while%30==0:\n",
        "                  loss = agent.step_distil()\n",
        "                  if loss != None:\n",
        "                    loss_episode_i += [loss.detach().cpu().item()]\n",
        "\n",
        "                # Update count_while \n",
        "                count_while += 1\n",
        "\n",
        "                # Update state\n",
        "                state = new_state\n",
        "                state_tensor_global = new_state_tensor_global\n",
        "\n",
        "                # Update analysis lists\n",
        "                list_state += [state]\n",
        "                nb_available_car += [len(jobs['104'])+len(jobs['044'])+len(jobs['444'])]\n",
        "          \n",
        "          # Keep on with the model only if it's the best.\n",
        "          if score<best_reward:\n",
        "            agent.Q_net.load_state_dict(best_model)\n",
        "            agent.target_net.load_state_dict(best_model)\n",
        "          else:\n",
        "            best_reward = score\n",
        "            best_model = agent.Q_net.state_dict()\n",
        "\n",
        "          all_scores.append(score)\n",
        "          all_loss.append(np.mean(loss_episode_i))\n",
        "\n",
        "          pbar.set_description('score=' + str(score))\n",
        "          pbar.update()\n",
        "\n",
        "          # Store the results\n",
        "\n",
        "          if i%5==0:#Gather every 5 episodes\n",
        "            # Save model \n",
        "            name = 'episode_' + str(i) + ' Distil'\n",
        "            agent.save_model(i,name)\n",
        "\n",
        "            # Get results for the analysis\n",
        "            name_score = name + '-score'\n",
        "            np.save(name_score,all_scores)\n",
        "            name_loss = name + '-loss'\n",
        "            np.save(name_loss,all_loss)\n",
        "\n",
        "  plt.plot(all_scores)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "G4GcdUcFxF4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load summary\n",
        "\n",
        "scores = np.concatenate((np.load('episode_20-score.npy'),np.load('episode_25 DeepQ_notExact-score.npy'))) \n",
        "plt.plot(scores)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSsWnKqDcQ3A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "60cc0974-0728-43c0-af29-5e30c936f087"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9d5Rkd3nn/Xkqh85pctSMNIzCjKRBEiJJwsgCGwvZGJCFLQNr7NfG62Ozi7H3GLDXfg84LF4bjFdeZAn2tUBk2SuQhCQQKCBGGoUZjUaTeqa7p3Oo7q4cfu8f997q290VblV1qq7f55w+Xf27t6puhb7PfdL3EaUUGo1Go2lsXKt9ABqNRqNZfbQx0Gg0Go02BhqNRqPRxkCj0Wg0aGOg0Wg0GsCz2gdQLV1dXWrnzp2rfRgajUZTVzz33HNjSqnuhet1awx27tzJ4cOHV/swNBqNpq4QkXOF1nWYSKPRaDTaGGg0Go1GGwONRqPRoI2BRqPRaNDGQKPRaDRoY6DRaDQatDHQaDQaDdoYaJaIvokYPzwxstqHodFoqsSRMRCRu0VkRESOLlj/fRF5VUSOichf29b/REROicgJEfl52/ot5topEfmEbX2XiPzUXP+aiPiW4sVpVo7//eMz/P59R1b7MDQaTZU49QzuAW6xL4jIjcCtwAGl1KXA35rr+4H3A5ea9/knEXGLiBv4AvAOYD9wu7kvwGeBzyml9gCTwIdreVGalWc6kWEmkSGX08OS1hv9kzG+8kzBplXNOsKRMVBKPQFMLFj+f4DPKKWS5j5WjOBW4KtKqaRS6ixwCrjG/DmllDqjlEoBXwVuFREBbgK+Yd7/XuDdNbwmzSowk8gAEEtnV/lINEvN137Wx5995ygzifRqH4pmGaklZ3Ax8GYzvPMjEXm9ub4F6LPt12+uFVvvBKaUUpkF64sQkY+IyGEROTw6OlrDoWuWmtmkcaKYTWTK7KmpNy5MJQDD+9OsX2oxBh6gA7gO+K/A/eZV/rKhlLpLKXVIKXWou3uR6J5mFZlNZub91qwfBiNxAKbj2jNYz9SiWtoPfEsppYBnRSQHdAEDwDbbflvNNYqsjwNtIuIxvQP7/po6IZrMmr+1MVhvDEZMz0Abg3VNLZ7Bd4AbAUTkYsAHjAEPAO8XEb+I7AL2As8CPwP2mpVDPowk8wOmMXkceI/5uHcC363huDSrgJUz0J7B+kIpxYUp0zPQYaJ1jSPPQETuA24AukSkH/gUcDdwt1lumgLuNE/sx0TkfuAVIAP8nlIqaz7OR4GHADdwt1LqmPkUfwx8VUT+EjgCfGmJXp9mhcjnDLQxWFdMxtIkMzkAItozWNc4MgZKqduLbPpAkf3/CvirAusPAg8WWD+DUW2kqUMy2RyJtHHC0GGi9YXlFYAOE613dAeypmasfAFoz2C9YeULAKZ1aem6RhsDTc3MJOdOEtoYrC+sSiKA6bj+bNczdTsDWbN2sHsGOky0vrgwlcDrFjrDfu0ZrHO0Z6CpmVm7Z6ArTopy71O9fP1wX/kd1xCDkTgbWwO0hbw6Z7DO0cZAUzMzNgMwm9RyFMW49+levn64f7UPoyIGpxJsag3SEvRqz2Cdo42BpmasPIHbJTpMVASlFEORBJOx1GofSkVciMTZ3BqgJeDVOYN1jjYGmpqxDEB3k18nkIswncgQS2Xryhjkcorh6QSb2oK0BD26z2CdoxPImpqxwkQbWgPaGBRheNoo0ZyKpVFKscwyXkvC2GySdFaxuTVAPJXVYaJ1jvYMNDVjGYANzX4dJiqCVa+fySlm6uQ9umAes5UzmE3qeRXrGW0MNDUTTWYI+dz5E4ZmMUO2ev2paH1cYQ+a3ceb2gK0BDwoRd0YMk3laGOgqZnZZIaw30OT36ONQRHsnbwTdZI3sDyDzaZnAFqSYj2jjYGmZmYSGZpNYxBNZjD0CjV2rJwBUDdJ5MGpOAGvi7aQl5aAaQx03mDdoo2BpmaiyQxNAQ9hv4ecgrgefbmIwUiCZr9RrzFVL8YgkmBzaxARoTXvGWjPb72ijYGmZmaTGZr8Hpr87vzfmvkMRRLs29QMwGSd5AwuROJsagsA0BI0DJkuL12/aGOgqZmZhJkzCBgnjKjuQl7E0HSCizc045J6ChMZ3ceADhM1ANoYaGpmNmnkDMI+wxhofaL5xFNZpmJpNrcFaQ1668IYZLI5RmYSbGq1PAOdQF7vaGOgqRkrZ9BkxsR1mGg+Q9NWvX6A9pCPydjaP6EOzyTJKfKeQbPfg4gefbme0cZAUxNKqbnS0nyYSJ8w7AyZJZobWwz1z3pIINt7DABcLqHJ79GewTqmrDEQkbtFZMScdWytfVpEBkTkBfPnneb6Hba1F0QkJyIHzW0/FJETtm095rpfRL4mIqdE5KcisnN5XqpmOUhmcqSziia/UU0E2jNYyNC0cWLd2BqgI+xjog4SyPYeA4uWgFYuXc848QzuAW4psP45pdRB8+dBAKXU/2etAb8OnFVKvWC7zx22+4yYax8GJpVSe4DPAZ+t+tVoVhzLC2jWYaKiWA1nxlwAX116BgCtQa1cup4pawyUUk8AE1U89u3AVx3sdytwr3n7G8DbpB5UvDTA3Ik/7JszBjpMNJ/hSIKWgIeQz0N7qD4SyIORBE1+T76KCIzyUh0mWr/UkjP4qIi8ZIaR2gtsfx9w34K1fzVDRH9mO+FvAfoAlFIZIAJ0FnpCEfmIiBwWkcOjo6M1HLpmqbAUS5sCHkI+NyLaM1jIYGSuRLMt5CORzhFPre3y2wtT8XwlkYUOE61vqjUGXwQuAg4Cg8Df2TeKyLVATCl11LZ8h1LqcuDN5s+vV/qkSqm7lFKHlFKHuru7qzx0zVJinfiNahMh7NP6RAsZmk6w0TyxdoR9wNrvNRgy5xjYaQnq0ZfrmaqMgVJqWCmVVUrlgH8Brlmwy/tZ4BUopQbM3zPAv9nuMwBsAxARD9AKjFdzXJqVxwoJWZVElj6RZo6hSIKNLYYxaA8ZYZe1bgwuTCXYXNAz0J/teqUqYyAim2x/3gbYK41cwHux5QtExCMiXeZtL/CLtvs8ANxp3n4P8JjSSmd1Qz5nYOYLwn639gxspLM5RmeTec+gLWR4BlNruNcgmckyNpvMh7YsWoKG15fJ5mp6/FMjMzzxmg7zrjXKTjoTkfuAG4AuEekHPgXcYJaMKqAX+G3bXd4C9CmlztjW/MBDpiFwAz/A8CgAvgR8RUROYSSq31/LC9KsLFbOwBJhM2Ss13Y8fCUZmUmiFPn4e3to7YeJhiNJYH4lEcxJUswmM3mjVg3/9PhpfvjaKM//2durP0jNklPWGCilbi+w/KUS+/8QuG7BWhS4usj+CeBXyx2HZm2yKEwU0GEiO1bD2QbLGIStMNHa9QwumIN4Ni/wDOzKpbUYg6l4moloKi9wqFkb6A5kTU3MJjO4BIJeQ7E07PNobSIbQ5E5KQqAtqDpGUTXrmcwGFncYwA2faIaK4qsJHTfRKymx9EsLdoYaGrCUiy1KoX1tLP55E+sLcZVts/josnvWdNhogtTi7uPAVoCSyNjbRmT89oYrCm0MdDURNRULLVoCniIprQxsBieThD0uvPzAABTn2jthokGI3HaQl6CPve89aVSLrW6mBvBM0hlcvzh117g6EBktQ+lLNoYaGpi1lQstQj7jTCRLggzGIwYPQb2pvqOsG9Newb2OQZ2lipMZHkW/ZPxmh6nFmYSaU6NzC778zxzZpxvHxng+0eHlv25akUbA01NWIqlFk1+D5mcIpmprfxwvWDvMbBoC/nWdM7gQmRxjwHMhYlq0SdKZXL5sair6Rl84fHT3Pr5n9RcJluOh18xjEDveHRZn2cp0MZAUxMzifkVIVqfaD6GFMX8E6uhT7S2w0QLk8dgFAe4pDbPYMZ239XMGbwyOE00lV3WY1BK8YNXDD3Oc+NrPySmjYGmJqLJDM0LwkSg9YkAcjnFyEwiX1ZqYQy4WZuegTWVrVCYyOWSmiUprA7mzrCP/sn4qoUTTw3PGL+XMVT08kCEoekEnWEfvePRNR861cZAUxOzyUx+3CVAk9+dX290xqMp0llVwDPwMZPIkF7mEEU15HsMCngGULskhWVI9m9uIZ7OMja78kZxNpnJz2s4Nbp8xuDhY8O4XcKvXbudmURmTXuDoI2BpkZmE/MTyE1+I8kY1V3I8yac2bEaz9ZiRdHglNUXsdgzAEOSopbSUivEdNmWVgD6Jlc+fHLa5g0sp2fwyCvDHNrRzsFtbcDazxtoY6CpGqUUs6n5paXhvGew9k50K401+3hj6+IEMrAmh9wU6z62aAnUFiayDMmlm1uA1UkiWwZgc2tgnmFYSs6NRzkxPMPNl25kR2c4v7aW0cZAUzWxVBalWFRNBGh9ImAoMjfu0s6ccunaM5iWZ7Ch1V9we60zDaxKpEs3m57BKhiDkyOzeN3Cjft6OD26PLH8R14ZBuDm/RvY1hFEBHrH1nYSWRsDTdXMLtAlst/W1URGJZHHJXSF559Y17JY3WAkTleTH7/HXXC7Me2shpyBaUg2tPjpavLTN7HyvQanRmbZ1RXmko3NzCYzDE8nl/w5Hn5lmH0bm9nWEcLvcbO5Nag9A836JW8M/AWqibQ+EUPTCTa0BHC55k9xbQ+vXX2iC5FE0eQxLIVnkMbjEoJeN9s6gquSMzg1MsPenmb2dDeZfy9tqGgimuJw7wQ379+QX9vZFeLsGi8v1cZAUzXWCX9eaalPl5ZaDEUSi0JEsNbDRIvHXdppCXqJpbJVV0JNJ9K0BL2ICNs7QituDBJpo7fgop4m9vRYxmBmSZ/j0ePD5BS8ff/G/NqOznBFnsGnvnuUP/nWSys6HlUbA03V5Afb2EpL3S4h5HPrMBHFjUHQ68bnca3JBLJ9XnMhLBnrmSo9v0g8k+9k3tYe4sJUYtm7gO2cHYuSU7C3p4nuZj/NAc+Sl5c+/Mowm1oDXLalJb+2szPEVCzt6DPP5hRfO9zHfc/28d7/9XRe7HC50cagQUmksxzunajpMawTgj1nAKY+UYMbA6WUMfu4ZbExEBE61mDj2XQizWwyU8YzsCQpqvNqpuPpvMbRto4g2Zxi0CzBXQmskNCeniZEhD09TUsaJoqnsvz45Chv379hnh7VXEVReU/o7FiURDrHbVdu4czoLO/6xyd57tzkkh1jMbQxaFDuP9zHr/6vp5moIW5tXf03m70FFs3aGDCdyBBLZYueWNtCXiaiaytMlO8xaCvuGVjTzqrtNZhOpPPexbb2ELCyFUUnR2ZxCezqMk7Oe7qbODWydIndn5waI5HOcbMtRARzz+ek1+D44DQA/+nNu/j2772RsN/N7Xc9w/2H+5bsOAuhjUGDcmY0ilIwEa2+kmJu/vH8ypOwf31MO3t1aJofnhip6r75hrMixqA95FtzYaK5HoPSOQOoXp9oOp7OG5RtHaYxWMG8wamRGbZ3hAiYw5j29DQxNpskskT5m4ePDdEc8HDt7o5569vN1+rEMzg+OI3HZXgtF29o5ru/90Zev6udj3/jJf7i319ZtrCaNgYNSr/5D1hLF2yh0lIwjMN68Az+/pGT/Mm3Xq7qvvmGswJhIjC6kNdamKgSz6Da8tLpRCYfatrUGsDtkhUtLz01Msuenub83/kk8mjtSeRsTvHoqyPceEkPXvf8U2vA62ZTa8CxZ7Cnpylf3tsW8nHvB6/hN6/fyd1PnuWD9/xsWS4kyhoDEblbREZE5Kht7dMiMiAiL5g/7zTXd4pI3Lb+z7b7XC0iL4vIKRH5BzEDaiLSISKPiMhJ83f7kr/KZeSVC9O83L/2B1csxNKSr0VaYDaZwed2LapJb/J710XT2fmJGOOzqaqakoo1nFkYnsEaCxNF4rgENjQXbjgDW85gCTwDj9vF5rbAiqmXZrI5zo5F8wYAbMZgCfIGz52bZCKa4uZLNxTcvqMz5NAzmOF1m1rmrXncLj79S5fy179yBT89O8GRvqmaj3chTjyDe4BbCqx/Til10Px50LZ+2rb+O7b1LwK/Bew1f6zH/ATwqFJqL/Co+Xfd8MnvHuWTDxwtv+MaQim1NMZggS6RRZO//quJlFL0TcRIZXNVeTmDkQQi0NNc3BhMxlLkcmtHyfLCVIKe5gAed/HTwpxnUPn3JpHOkszk8qEmMPIGKxUmOjcRI51V7LUZg63tIXwe15IYg0deGcLrFt56cXfB7TsdlJdORlMMTSd43abmgtvf+/pt/PjjN3LjJT01H+9CyhoDpdQTQE1lJyKyCWhRSj2jjMusLwPvNjffCtxr3r7Xtl4XnB2LMjqz9B2My8lULJ0/wdUaJlqYL4D1UU00FUszY76G8SqUNYenE3Q1+fF5Cv+LtYW85FT1JZrLQbE5BnZCPjcel1TlGVj3abFdQGxrD61YmOjk8FwlkYXbJezuCtdsDJRSPPzKMNdf1EVzwFtwnx2dYcZmU/NmOizESh4v9AzsbCgSeqyVWnIGHxWRl8wwkj20s0tEjojIj0TkzebaFqDftk+/uQawQSk1aN4eAgr7WICIfEREDovI4dHR0RoOfWmYSaQZj6ZqqshZDezjBmsNEzX5F3/xmwL1bwzsV6vjVXy+gwUmnNlZi5IUg5FEUYE6CxFrpkHln691n3meQUeQsdnkijRXnTb7CS6yGQMwjEOtvQYnR2Y5Nx7j7fuLnr7Y2Vk+ifyKA2OwXCz28Z3xReC/A8r8/XfAh4BBYLtSalxErga+IyKXOn1QpZQSkaJ+s1LqLuAugEOHDq26f219qLFUlkQ6m69QWOv02050tYaJ7IqlFk0+D6lMjlQmV/TKeK1jj2NXY+yHIol8tUwhOsJzxmAn4coPsAznxqP0TcQZjMQZiiQYmk4wFEkwGEmgMCqGNrcF2dQWYEtbkE2tQQYjcd62r3z4oSVQnYx13jOYZwzmKoou3lA4NLJUnByeYXNrYJ58ChjG4P++PFjT/7AlTFfKGNh7DSwJ74UcH5yhu9nQbVppqjIGSqlh67aI/AvwH+Z6Ekiat58TkdPAxcAAsNX2EFvNNYBhEdmklBo0w0nV1fKtAnYLPx5NsaVEFcZawrrqbQt5a/YMupp8i9bDttGXPs/i7cvF948O8eWne7n3Q9csquaoFHvoopry26HpBNfs6ii6vS0vSbH0nsGPXhvlzrufnbfWHvKysTXIxhY/IsLAVJzD5yYXff6lDJhFS7A6fSIrz9ASKGAMJpbfGJwanWVPgefY09OEUobnYKmpVkIup/jW8/1ctb2tZAhnh+kZlKooOj44vSpeAVRpDKyTt/nnbcBRc70bmFBKZUVkN0ai+IxSakJEpkXkOuCnwG8A/2je/wHgTuAz5u/vVv1qVhj7hzo+m6wbY9A/Gacl4GFre7BmY7Cza/FVrZVUnk1m8qJsy41Sir99+ASnRmb5We8E11/UVdPjnZ+I0WTmPioNE1mjI0udGPJhomVoPHvq9Bhet/DlD13L5rYAG1oCRa94o8kMg5E4A1MJJqLJeXo6xah2poE1Ia01OD9nAMvfeJbLKU6NzPJr13Qu2mavKKrGGPzk1BinR6N87n0HSu4X9nvobvbTO1bYGKSzOU6NzPLmi2v77lZLWWMgIvcBNwBdItIPfAq4QUQOYoSJeoHfNnd/C/AXIpIGcsDvKKWs5PPvYlQmBYHvmT9gGIH7ReTDwDngvTW/qhXi/HhtceXVom8ixtb2EK1Bb031ykbOoFA1kekZpFYub/Cj10bzScCHjw3XbAz6TDGz14ZmmKgwgWz1GJSSdVjOnMGR81Ps39zKGy5afOJbSNjvYU9P87za+3K0BD3511gJkQKeQVeTj6DXTd9k6STysQsR/sfDr/E/b7+y4HeuHANTcRLpHHs3NC3atrMzjEuoetDNvU/10tXk552Xbyq7784S5aWnR2dJZXPsX6uegVLq9gLLXyqy7zeBbxbZdhi4rMD6OPC2csexFukdj9LV5GNsNlXxCWM16Z+Ms6srjNftYigyXfXjzCYy+ZnHdlZDxvruJ3vpafbzuk0tPPLKMJ961/552jCV0jcZ44qtbYzNJCvOGQyW6TEAQ+nV7ZIl7zXIZHO83B/hfa/ftqSPa6dqzyC+OGcgImxtD5btNbjv2fM8+uoI3zjcx2++cVfFz23XJFpIwOtmW0eoqiRy71iUx06M8J9v2lt0BoSdnZ1hfvRa4eIXJ5VEy0l9ZvfWCOfGY1y53SikqpeKIqvHYFtHiJagl0iVnaSZbI54Olu4msi/sjLWJ4dneOK1UX7jDTv4hSs2MTAV59iF6o1cJptjYDLO9o4gnU2+ir2+4TLdxwAul9AW9DKxxJ7BieEZ4uksV25vW9LHtdNabc4gkcbncS0KWW3vCJUMEymleOy4kUq89+lzVfVm5I1B92JjYK2frkKj6MtPn8PjEu64druj/Xd2hRmZSRIr4DUfH5zB53Gxu0DodSXQxqBKEuksQ9MJLtvcitctdRMmGo+miKezbG0P0hr0EolX12EbNUsBCzedWQnklelCvvvJs/g9Ln7t2h28bV8PLjE0YqplMJIgk1Nsaw/REfZV4RmU1iWyaAvVFqYrxJHzRmfqVduXr5G/Jeglkc6RzFT2+U7HM/NCRBbbOkL0T8aLfg9fHZrhQiTBm/Z0cXYsyhMnKy8rPzkyQ1eTr2gOa09PE2fHohXp/swmM3z9cB+/cPkmehzW/u8oUV56fHCaizc0lWz6W060MagSy63d2WWdMOqj8czqMdjWHqIt5CWdVcTTlZ+0Z/OKpYuNgdWINptcfrmFiWiKbz0/wC9ftYWOsI/OJj+Hdnbw8CvD5e9cBKvaantHdcZgKJKgNegl5CsdhW0P+ZY8gXzk/BRdTT62ti9fMYPVNFZpr4Ex2Gbxe7K1PchsMlM0ZPbYq4ZX8Nn3XEF3s597nuqt7ICxNIkKewVg9B6ksrmyuQs733q+n5lkpqKw1c58eeliL+T44DSv27g6ISLQxqBqLMu+ozNMR9hfN2Eiq8dga0cwLyVcTUWRlQ8IFzAGlqT1SugT/dtPz5HM5PiQ7R/y5v0beHVopuqZs1bIYltHiM6wj/FosiLvaahMw5lFe3jpZxoc6Zvk4Lb2mvIl5ahWudSuS2SnnHrpo8eHuWJrK1vagnzg2h388MQoZyqI7yulOFnGGFSqUZTLKe55qpcD29o4uM15SG57vrx0/msdmUkwNptatXwBaGNQNdaJZkf+hFEfxsCqn7eqiaA6SQrrqr9QmMjyDJZbnyiVyfHlp8/xlou72WurH7e05B+p0jvom4jjdgmbWgN0hP0k0jliFXTIDk0XnnC2kPbQ0iqXTsVSnBmNLmu+AKrXJ7IPtrFjlZcWSiKPzyY50jfFTWYz3K9du90om336nOPnHZ1JMpPIsLdExVSlxuDHp8Y4Mxrlg9fvdHwcYLx3nWHfoguV44OGaqo2BnVI73iUloCHtpC3qlDCatE/GaM95KXJ76GtFs/AvOovVObncbvwe1zLnkD+j5cuMDKT5ENv3DlvfXtniH0bm3n4WHXG4PxEjC1tQTxuF51mjLmSz9cYHenEGPiYjKWrytkU4gVTyXLZjUFeubTSMFFmni6RxbYOI6RVSKPohydGUQrets/o7O1u9vOuKzbz9cN9JTV+7JwsUUlk0RLw0tPsd2wM7nnyLN3NzspJF7KjM0Tv2HzDZ1USrVZZKWhjUDXnxmPs7AobIwzDvropLe2bjLPVvBJrWYIwUbGa7+Zl1idSSvGln5xlT09TQZXIn790I4fPTTA2W3ku5/xELH+CsmQjnHp+6WyOsdmkIzGxtpCPVCZXVc6mEEfOT+ESuGLr2vUMWgt4Bs0BL+0hb8Ew0WOvjtDT7OfSzXMnyTuv30k0leWbz/Uv2r8Q1gl+bwljAM41is6ORXn8xCh3XLu9KrmVQuqlxwen2dwaoDVUWORuJdDGoErOjcfy04s6wz5mkpmKqytWg/7JWD65aEkiVDPlqVSYCJZ/2tmzZyc4dmGaD71xV8H4+M2XbiCnyJckVkL/pO2zbbI8A2dGZWQmiVKlG84sOsKWJMXSJJGP9E1x8YbmqpqyKqG1ipyBUspMIBc+2W0rUF6azuZ44rVRbtrXg8s19xkf2NbGldvbHJeZnhyZoTlgdP+WYk9PE6dHZst6avc+1YvXLfyaw3LShezoDHMhkiBhuwg4PjjNvlX0CkAbg6pIZ3MMTMXzlQGdpqjUWg8VKaUYMHsMgNoSyCXCRABhn2dZm86+9JOztIe8/PJVWwpu37+phS1tQR6qsMQ0mswwNpvKv0edYeOzdSpjXW6ojZ22vCRF7d+bXE7xwvnJfN/LcpJPIFdQTZRI50hnVcEEMlhS1vONwc96J5hJZvL5Aju/ef1Ozo5F+ZGDMtNTI7Ps7Wkqm1Tf09PEbDLD8HRxwz+TSPON5/r5xSs2F51VUY6dXfMlOBLpLKdHo0VnGKwU2hhUwcBknGxO5WuG86GENR4qGp1Jkszk8p5Bk9/sgo1XftzlwkTLKWN9bjzKI8eH+bVrtxfV3BERbr50Az8+NVaRh2KFKqykZkdTZTmDoYhxInGWQF46SYozY1GmE5llzxcA+D0ufG5XRZ5BXoqiQGkpGNVtA1PG/5XFY8dH8HlcvHHPYmmRd1y2iZ5mP/c82Vv2ucuVlVpYDWml8gbffK6f2WSG36wwcWzHUi+1KopOjcySzalVTR6DNgZVYQnU7ch7BpUnGVeDPluPARgnTKPxrLowUdDrxu0qfLXV5PcsmzbRPU/14nEJv/GGnSX3u3n/RlIZI9TgFCuJaYWJwj43Po/L8Webl6JwUloaWrow0ZHzkwBcWUGZY7UYMw0qk7GeG2xT3DNIZ1W+exuMfMEbdncWLF/2eVzcce0OfvTaaH5OQSEmoynGZlMlK4ks5iqKCs9DzuUU9z59jiu3t3Gghvd5l2UMTMG61ZxhYEcbgyqwegx2LvAM1roxyPcY2BqSWquUpJhNZovmC8CcdrYMYaKBqTj3/6yPX7xic9kk7et3ttMW8lbUgHbe1mMAxomvktLhoUiCoNddMFG6EKsbdim6kI/0TdHs93BREbmFpaZSfSJr3++4ZP4AACAASURBVGLvy/aO+aGTM6OznBmL8rbXFZ+vYJWZfqVEmamVEHbiGXQ3+2kOeIomkf/5idOcHYvW5BUAtIa8tIW8+YvK44PTBLyufNh5tdDGoArOjccIet35hFRnhRUnq4XVfbzFZgxaqlQunU0WHmxjYcg/L21CfTqR5kP/+jNcInz0pj1l9/e4Xbxt3wYePT5M2qHMQJ8pXd1uq+qopHR4cNooK3XS9GWV9i7FRcSR81Mc3N42L9G6nDQHvRWVlhYabGNnrvHM+I5aXcelZv06KTMtNOqyGCJiVBQtCBMppfjM917lr79/gl+4fBO/UEU56UJ2dIbzF5XHB6e5ZGNLUS97pdDGoArOjUfZ0RnK/8O3BLy4XbLmJSn6J2N0NfnmySS0BatToJxNpAu67xZNfveSVhOlszl+9/88z+nRWb74gasdXwHffOkGphMZnj3rbIx330SMbR2heSfzjgo9g3JzhC08bhfNAU/NyqXRZIYTQ9MrEiKyaAl4KvQMMvn7FWJzWwCROc/ssVdHuGRDc9lhO1aZ6ZefPlewCujUyCxBr9vxrJE93U2csgnWZbI5PvHNl/nnH53mjmu38w+3X7kk2kE7O0P0jkdRSnF8cIb9q5w8Bm0MquLcRCyfPAZDgbI9tPYbz/om4mxpn//PVX3OoPAsA4uw30M8na1I+KsYSin+27df5ienxvh/f/ly3rTX+ayCt+ztJuB1ORauOz8RY9sCXZ/OCrSnDCkK57pARuNZbd+bl/oj5BQrUklkUalyaTnPwO9xs7ElQP9EjOlEmmfPTnBTiRCRxYFtbbxhdyd/89AJ3vX5n/DN5/rnlXifHJnhop6wY49pT08TY7NJIrE0iXSW3/u35/na4T7+8017+Mt3X7ZkV+87OsNcmIpzfiJGJJ5e9XwBaGNQMdmc4vx4bFF8rzPsW/PVRPYeA4vWoJepKktLS+UM5gbc1B4q+sLjp7j/cD//+aY9vPdQZTr9QZ+bt+zt5uFXhsvWjyul6LP1GFh0hP2OPttszkiAOukxsDD0iWrzDI70GcnjSjRyaqUl6K2otNTqZWku8Z3Z1h6ibzLGj18bI5NTjuYxA9z9m6/nr267jEQ6x8e+/iJv/Mzj/P0PXmN0JsnpkdmistWFsMJJL/RP8cF//RkPHTNmY/zRzZcsqd7Tzs4QOUW+S14bgzpkaDpBKpvLC05ZrHVJilxOMTAVz1cSWbSFjDBRpRrxs8l0Sc+gyTYHuRa+c2SAv334NW67cgt/+PaLq3qMmy/dyGAkwcsDkZL7jc4mSaRzi0ITnU0+YqnsvCahQozPJsnklKOyUov2kLfmPoMj56fY1RVesRGjYCaQK/QMAl5XyQEwRuNZnEdfHaYt5HXs6QR9bu64dgeP/OFb+MqHr+HyLS38/Q9O8sbPPMaFSGKeblU5rPDj7/6f5/hZ7wR//76DfLCKYTrlsCoRv3fUmB68b+Pqh4mWt1VxHWK1kS/0DDqafByvYaDKcjM8kyCdVQU9g5yC2VRhrfliGFPOSoeJoPSAm6MDEX7ry4fZt7GZ6y/q4vo9nbxuY0vepX/mzDgf/8ZLXLe7g8/8yuVVX5nNzTgYLinVYFWyLPYM5goESsWerTkGFXkGIZ9jPZxCKKU4cn6Kt1QQOlsKWoIeUpkciXS2aK+Hnel4pmyF1baOIEPTCR57dYQbL+mpOCQjIrx5bzdv3tvNmdFZ7n2ql4eODXO9g/Gfc8cQwu9xkVWKf/mNQ9zo0DupFKsS8fnzU2zrCNJcwf/ecuFkBvLdwC8CI0qpy8y1TwO/BVgF3H+qlHpQRN6OMdPYB6SA/6qUesy8zw+BTYClRnWzUmpERPzAl4GrgXHgfUqp3iV5dcvAnHT1gqvHNa5cmp9j0LE4ZwCGG1+JMYiWCxMFyhuDZ86MMxhJEPC6efzEccC4Un7DRZ1ctb2df3zsFNs6gvyvDxxyNFKwGO1hH4d2dvDoqyP8l5+/pOh+Vo/BwvcoXzo8W84YOO8+tjAG3FQfJuqfjDM2m1yRZjM7dn0iR8YgUf77ZXmtU7F0zSfh3d1N/Pmtl/Hnty6atFsSt0v44geuoqc5wGVbWms6hlJ0hH00+z3MJDOrOsPAjhPP4B7g8xgnbDufU0r97YK1MeBdSqkLInIZ8BBg1wu4w5yFbOfDwKRSao+IvB/4LPA+py9gpekdj+J1C5ta558UOsI+IvE06WwO7ypNKipFoR4DmC9J4TQan8xkSWVzNYeJzoxFaQt5efy/3MBgJM5Tp8Z58vQYT50a58GXh+hq8nHPB69ZEvGum/b18JnvvcpgJL7os7OwKlkWvkddTZZnUDqJPOcZOE8gd4R8zCYzpDK5qkTPjuSVSlcueQzzZxo4mfJVSpfIwjLCbpfw1r2LxQdXiptMhdTlRETY0RXi6MD0msgXgANjoJR6QkR2OnkwpdQR25/HgKCI+JVSpf6LbgU+bd7+BvB5ERG1VLq+S8z5caP0cKELa+kTTUZTjkfgrSTWVe/CK9tq9InKSVGAoU1k37cQvWPRfLhtU2uQX7l6K79y9VaUUvSOxwj53I7UP51gGYPHXx0tKjDWNxFjQ4t/0ZVuR9iZ9tRQJIHP45rXo1CONlvjWTXfmyPnJwl4XVyywjFnq0TUacNiJJ6mu6m0UJylFHtoR/uqqneuFDs6w2vKGNRyCftREXlJRO4WkUKXJb8CPL/AEPyriLwgIn8mcwHgLUAfgFIqA0SAgkE+EfmIiBwWkcOjo5XPQV0KegtUEsHqNp79zUOv8v67ni65T/9kjJ7mxSc6SyytklBFtIxIHcxVjZQKE/WORdlVYPi3iLCrK7xkhgAM+eItbcF8M1Mhzk8sriQC5x3m1hyDSnIbtUpSHDk/xRVb2lbcG6102tl0PFPWM9jQHODyLa287/WVVYzVK5YsxWrOMLBT7Tfoi8BFwEFgEPg7+0YRuRQj3PPbtuU7lFKXA282f3690idVSt2llDqklDrU3b3ybqRSivPj0ZpOGMvBT89M8MyZCU4MFdZUAcMzKDQXtxrPYKaMfDXMJZCLhYkS6SwXIomCxmA5EBFu2tfDk6fGilYF9U3EFlVbgXEV7HVLWUPvdNylnVrE6pKZLK9cmF7xfAHYZKwdfm+c5AxcLuHff/9N/PJVW2s+vnrgPVdv5eO3XJL3iFabqoyBUmpYKZVVSuWAfwGusbaJyFbg28BvKKVO2+4zYP6eAf7Ndp8BMMLVIuIBWjESyWuOsdkU0VQ2XwlgZzU9g3NmrPuBFweK7tM/FSvYzZkffVmBcqmjMJE5+rKYZ2DpsuxcIWMAcOO+buLpLD8t0I2cyuQYnE4UfI9EzKbCMr0Gg9PxiiqJYM4YVCMJcuzCNKlsblWMQT6B7ECSQilVdLBNI7OzK8zv3rBnWedVV0JVxkBE7OIctwFHzfU24P8Cn1BKPWnb3yMiXeZtL0Z10lFz8wPAnebt9wCPrdV8wbkFaqV25ipOVlaSIprMMDpjPOe/vzhYsLEqk80xOJUo6BkEvC58HldlOYNkeWPg97jxuV1F9YksxcZdKyjO9YbdXfg9Lh4vECoamIqj1OJKIotykhS5nGI4kmRjBcljgPawpU9UeZjoyPnVSR7DXBjQiWcQTWXJqeLy1Zq1QVljICL3AU8Dl4hIv4h8GPhrEXlZRF4CbgT+0Nz9o8Ae4JNmbuAFEekB/MBD5v4vYHgD/2Le50tAp4icAv4I+MQSvr4lpVhZKRixd5GVDxNZFTA3XNLN+YlYfg6unaHpBJmcyo+7tGPJWFeiM5M3BiXCRGB4B8XCRGfNGbDWoI+VIOhzc/1FnTx+YmSR0TxfpMfAorOptCTFRCxFKpur2jOoJkx05Pwkm1sDS5pbcUrA68bvcTn63lj7VFK6rFl5nFQT3V5g+UtF9v1L4C+LPNTVRe6TAH613HGsBc6NR3EJBU+qblOfaKXDRJaB+sibd/PU6XEeePHCoivF/gVzDBbSGqys1t0yBqVUS8GUsS5qDGbpavKveLPNTft6ePy7xzgzFp0ndles4cyiI+znpcnFhtZiyCwrraTHAIyTasDrqjhMlM0pDvdOcvWOlfcKLFoc6hPNDbbRxmAts/YK4tcwveMxNrcFi9aDr4YkxfkJI9xy6ZZWbrykm/94aXDetCiYMwaFwkRgKJdWU1paSrUULBnrIjmDsRi7VtArsLCamRaGivomYvg8LnqKzMntDJfOGVTTfWzREapcn+ibz/UzNJ3gF6+oXU65Wgzl0vI5A+0Z1AfaGFTAuYnCZaUWlUgdLxW94zHaQ15ag15+6cAWRmeSPHNmfv69byKGCEWllStVLo0mM4hAyFe687TJ7ykeJhqPrsowj63tIS7e0LSoxLTPFPErpm7ZEfYxk8zMU8S0U8ns44W0hXwVeQbxVJb/8chrHNzWxi2Xbaz4+ZYKp56BlWTWCeS1jTYGFWDNMShG52p4BuMxtpsn1be9roewz80DL1yYt0//ZJyNLYGikg6tFUoizJjy1eWqIIqFiWbNpPdKVhLZuXFfD8+enZg3EOV8kbJSC6tAYLJIoncwksDjErrCpRurCtEe9lb0vfnXp84yNJ3gT96xb1UrUZzmmqbLzD/WrA20MXBIJJZmKpYuaQw6wj7GV7ia6NxElB1mnDvgdfPzl27kwaOD865gC0lX26k4gVxGpM6iKVDYGFiVRLtXyRjcdEkPmZziJyfH8mvnxws3nFnMlQ4X/nyHIgk2tASqmjRmeAbO3v+JaIovPn6at+3r4drdzgXYlgNDudRBmKjM/GPN2kAbA4ecmyheVmrRGfYxFU8vitkvF6lMjoHJ+Ly+h3cd3MxMIsOPTsx1aPdPLpauttMa9DKTzDgeRFNusI1Fk69wmOjs2Mr3GNi5ekc7LQEPj58wQkWRWJrpRKa0MWgqLUlhdR9XQ0cFA24+/9gpoqkMf/yOfVU911LSEnQ27czKK5SaZaBZfbQxcEivWbVTKs7d2eRHqerKBKthYCpOTpEPEwG8aU8X7SEvD7xohIrS2RyDkcLdxxZtQecNRGAaAwf/2GG/p6A2keUZrNYAcI/bxVsu7ubxE6PkcsZAG6BkJ2i5DvOh6URV+QIwJCkiDi4i+iZifOWZXt5z9VYurkCjf7loCRjHXa4tKBJPE/a5l2RcpGb50J+OQ86bDWelrh5XWpJirglu7pi8bhfvvHwTPzg+TDSZYXAqQU4VLoe1sETBnCaRHXsGAY/RcLTgJHd2PMrGlgDBMgno5eTGS3oYnUly7MJ0vseg1LzdfJioQEWRUspUQ63OGLSFfORU+Qauv334BG6XVD3kZ6lpCXrJ5BTxMkN/nCiWalYfbQwc0jtuKFqWOoGVOmEsB9ZJbMeCk9itB7eQSOd45JXhOenqEle9eUkKhx6N45yBKUkRW3CyOFtEoG4lueGSbkSMwet9DoxBa9CL2yUFDX0kniaRzlXcfWxhdSGX8iiPDkT47gsX+NAbd1Ukkb2czM00KO1RaimK+kAbA4cYlUSlT2AdTSvtGcQIet10L6iNP7SjnU2tAR548ULZhjOA1qBx3E49g6hDzyA/7WxBqKh3LLpq+QKLziY/B7a28diJEc5PxGgLeUsmOF0uoT3kLVg6XEuPAdi7kIu//5/53qu0h7z8zg0XVfUcy4FVHVSuvNSJSJ1m9dHGwCHnxmOLrsAXMhcmWpmKIqvUdWF5ocslvOvAZp54bZSXByK4XVLyRFWpcumMw5xBU4HRl5FYmslYelUazhZy074eXuqf4oW+qZLhPwujqXDxZ1vNhDM7ljG479nzvNg3tSis9sRro/zk1BgfvWnvmjqpOlUuNeSrdfJ4raONgQNiqQwjDurirX/qlWo8O1eiHPKXDmwmk1N847l+NrYESibvKjEGSinHnkGhaWdni8yQXg1u2teDUob6ZynPycIoHV56z2DvhibeenE33z4ywK1feJLrP/MYf/ado/z45CjJTJbPfO9VtrYH+cB1hYfyrBZzyqWlvzeRuPYM6gFtrh1QSqDOjtftojVYWQNRteRyivMTMW64pPBch0s3t7C7O8yZ0WjJSiKYPwe5HPG0oUBZUZjIbgzGjOHvu7tX3xhcurmFnmY/IzPJkvkCi86wn+OD04vWhyIJ3C6hp7k6YxDyebj3Q9cwFUvx2KsjPHxsmG88189XnjlHwOsikc7xP99/sKY50MtBfsBNuZyBTiDXBdoYOGAuUVv+BNa5QpIUIzNJkpncvLJSOyLCLx3YzN//4GTZE53P4yLkczvyDPKzDKoME50di+GS0snalUJEuPGSHr52uM9xmKhYzqCn2b9oFGqltIV8/PJVW/nlq7aSSGf5yckxHn5lCKXgXVdsrumxl4O50ZfFvze5nGI2mcnvq1m76DCRA8bMruKelvJSAx1lBM2WinP5cEvxk9gvHTBOIOVyHWD0Gkw5MAYzDmYZWBQKE/WORdncFlwzV7k/t98Yfu7EU+kI+4jE06QXNOcNRarvMShGwOvm5/Zv4K/fc4C/+dUDVXU2LzfNgfI5g5lkBqW0Ymk9oI2BAyypACflccbV4/InkPOhqxLeyu7uJu790DV84LodZR+vxaFYnZMpZxaFw0SrX1Zq5+de18N9v3Ud1+7qKLtvZ1Ph2QO19BjUMz6Pi6DXXTJnMK3lq+sGbQwcEImnCXhdi4bJF8IYgrICnsFEFI9L2FxEidTirRd3025WOZXCqXJptALPwJIfsIyBUoreNWYMRIQ3XNTpSPCtUFOh0XCWYGPL2qj9X2kMSYriOQOtS1Q/aGPggKlYirZg+RMqGCeMyVh6UXngUnNuPMaW9uCStfi3hbyOEsgzDqecAfg9LtwuyRuQ8WiKmWRmTVQSVUOnqUhqDwPOJDPEUtmG9AwANrYE8jm1QkS0YmndoI2BA6ZiadpCzq5sOsJ+sjlV0XyAajg/ESvbBFcJTj2DSsJEIkLY587fJz/3eA15BpVghYnsSeRqJ5ytFy7f2srRgUjRix/La9CewdpHGwMHTFXQTt9V4ISxHPSORR0lhp3iOEyUcm4MwEgyziYNOYozq6xWWiuFwkS19hjUO1dsbWMmmcl/tguxwkRajmLt48gYiMjdIjIiIkdta58WkQHb4Pt32rb9iYicEpETIvLztvVbzLVTIvIJ2/ouEfmpuf41EXEWk1khIhV5BssvSTEVSzGdyJTte6iEtpCPeDpbdJKXxUwFpaUAYb87HybqHTPyHOX6HtYq7SEfIgs9g9q6j+udA1vbAHipv/B8aJ1Arh+cegb3ALcUWP+cUuqg+fMggIjsB94PXGre559ExC0ibuALwDuA/cDt5r4AnzUfaw8wCXy42he0HEzFK8sZwPJKUliVRE5q453S4rALeTaZwesWx6Wh9mlnveNRtnWE8NaplLHbJbQFvfM+28FIAhGqbjird/b0NBHyuXmxr4gxSBgjUpsdepKa1cPRf6VS6glgwuFj3gp8VSmVVEqdBU4B15g/p5RSZ5RSKeCrwK1ilHHcBHzDvP+9wLsreA3LTiU5AyvJuJxhonNWE9wS5wygfBeyUykKiyabMTg7FivZF1EPLJSkGIok6Gry4/PUp4GrFbdLuGxLKy/2Rwpun46nafJ71mSfhGY+tX6DPyoiL5lhpHZzbQvQZ9un31wrtt4JTCmlMgvWFyEiHxGRwyJyeHR0tNAuS04inSWZyeU1/8thyREvZ+PZubHysxUqpc2pZ5BwJlJn0eQ3pp1ZZaX1mi+w6Az75xn6WiacrRcObG3llcFpUpnFk/KmtS5R3VCLMfgicBFwEBgE/m5JjqgESqm7lFKHlFKHursLa/IsNVbDmdMwkd/jptnvqdozGIokCo6KtHNuovxshUpxKlY3k8wQ9jk3BlaYaHg6STydXbW5x0uFoVw63zPY2NLYxuCKrW2kMjleG55ZtE3rEtUPVRsDpdSwUiqrlMoB/4IRBgIYALbZdt1qrhVbHwfaRMSzYH1NMBU3/vGdhonAmGtQaQI5Ekvz6QeO8cbPPsaffvvlkvueH4850kmqBOv1lRvMPpvIVDTL1goTrfbc46Vi4WfbqN3Hdg5uM5LILxTIG0zHM7TqHoO6oGpjICKbbH/eBliVRg8A7xcRv4jsAvYCzwI/A/aalUM+jCTzA8oYoPo48B7z/ncC3632uJaaOc+gAmNQgSRFJpvjK8+c44a/fZwvP93L9o4Q3zs6VPIK/dxElO1LHHt36hlEU5XnDKJ2Y1CnDWcWnWFjeH02Z0h5TycyVU84Wy9sbQ/SHvIWrCjSg23qB0f/1SJyH3AD0CUi/cCngBtE5CCggF7gtwGUUsdE5H7gFSAD/J5SKms+zkeBhwA3cLdS6pj5FH8MfFVE/hI4AnxpSV7dEpDXJarAM+gM+/ITxkrx9Olx/vzfj/Hq0AzX7e7gU++6lGQmx7u/8CTfPzrI+16/WL8+nsoyPJ1c8kSsJTrmJGdQSeI67PeQU3B8cBqf28Xmtvo+cXaEfShllPdak8ka3TMQEa7Y2sZLBZLI03EdJqoXHBkDpdTtBZaLnrCVUn8F/FWB9QeBBwusn2EuzLSmiOTDRM5bHzrCvoL/GBZjs0k++d2jPPjyEFvagnzxjqu45bKNiAhKKXZ3hfnW8wMFjYHV+l9Murpa3C6hJeBxlDOwZhs7wUo2H70QYUdnqGaZ59XG3kcyPG14f43aY2DnwNZWPv/4KLFUhpAtp6QH29QPjVkPVwHVhYn8TMZSGBGwxfz191/lB6+M8LG3X8yjH3sr77h8U14oTUR495Vb+OnZCQamFnsXlnT1UnYfW7SGynchV15aahiO44PTdZ8vAOhqmisdtsZdNrpnAHBgWxs5BUcH5ob/ZLI5oqms1iWqE7QxKMNUPI3XLYQqqNzpDPtIZxXTicVVQbPJDP/x0iC3XbmF33/b3oJKqO8+aFTWfufI4jx6ftDOMtTrl5OkyOYUsVSWJr9zw2hVHiXSubrVJLJj9wwsXaINDV5NBEZFEczvRLa61bVnUB9oY1CGqVia1qDPkcSxhSVoVqii6MGXBomlsrz39VuL3n97Z4hDO9r59pGBRd7FufEYLQFPRWErp7QFfUzFildBWc1j4SrCRFD/yWMwDD2YnsF0go6wz5G0+Xqnu9nP5tbAvOYzrUtUX2hjUIZIPFVRWSmUlqS4/3AfF3WHuWp7+6Jtdm67agunRmY5dmH+zN3e8eVr3CrnGVj9D5WWllqsB8/Amg0xMZvSPQYLOLCtbZ4sRV6xVBuDukAbgzJMxdIV5QvAJkmxoAv59Ogsh89N8t5D28p6Gr9w+SZ8bhffen5+qOj8RGxJO4/tlJt2Zm2rKEy0zoyB1+2iJeBhPJrU3ccLuGJrG+cnYkyaHvHcYBudM6gHtDEoQyW6RBYdRcJEXz/cj9sl3HZVQbWNebSFfNy4r5sHXrxAxpy5m8nmGJiML0u+wHhOwxgUS3xb8eBLNjY5fkxLoCzodbPBwQzpeqCzyZCkGIrEdSWRjQNbWwF4acAIFUW0YmldoY1BGSJxI2dQCfa4skUmm+Obz/dz4yU9jhUub7tyK2OzSX5yagyAC1MJMjm15N3HFq1BL+msIp4uLGP99Olxupv9XNTt3BhYnsGOzlBFeZe1TEfYx4WpOJOxtPYMbFxmGQMzVKTlq+sLbQzKMBWrPGcQ8LoJ+dzzPIMfnhhldCbJew8VTxwv5MZ93bQGvXzbrCrqtcpKl8szCBaXpFBK8fSZca7b7WxesEXI50YEdnfXf4jIoiPs49VBQ4dnU4N3H9tpCXi5qDvMi6YHqRPI9YU2BiVIZYw66UpzBmBJHc8lkO8/3EdXk58b9/U4fgy/x80vXLGJh44NMZvMLIt0tZ1SkhRnx6IMTye5bndHRY8pIhza0c4b93QtyTGuBTrDvrz3pD2D+RzY2saL/RGUUkzHM7gEwksoqKhZPrQxKIF1UqzUMwDjhGGFiUZnkjz26gi/ctWWige73HblFhLpHA8dHeL8eBS/x0VP8/LE3ksZg6fPjAPwht2dFT/u13/neu64dkdtB7eGsKrFQHcfL+SKra2MziQZmk7kFUvXS3hwvaPT/CWwpChaq6jp7wj7GJkxPIPvHBkgk1P8agUhIotDO9rZ2h7k20cGCPncbO8ILdugkNYSyqXPnJlgQ4t/XVQE1Yo2BsW5wlQwfbEvomcZ1BkN5xn80w9Pccf/fsbRvtVIUVh0hP1MRA1JivsP93HV9jb29DRX/Dgiwm1XbuHJ02O80De1bCEimPMMphd4Bkopnj49zhsqzBesV6ymwtagd54Ojwb2b2rB4xJe7J8ydIm0FEXd0HDGIBJP87Ozk0XLJ+3kjUE1YaImI0x0pG+KkyOzvPfQtvJ3KsJtV25BKRiZSS5b8hiKh4lOj84yNpvkDRdVHiJaj3SYfSQ6X7CYgNfNvk3NvNQ/xXQioz2DOqLhjEFPc4BUNld2iAsYukTgfMqZnY6wj1Qmxz1P9hL0Gongatnd3cQB0/1eTmPQ5Pfgdkl+oI/F06eNfMF1VeQL1iNW6bAOERXGkrOeiqV0JVEd0XDGwGp8suL5pbB0eiqZZWBhnTD+46ULvPPyTfl5AdVy28HNwPJVEoERkiokSfH0mXE2twaWrfO53rDCRNozKMzBrW3MJDL0jse0Z1BHNFxAz2r4GplJcMnG0jH8SDyNS+a6aCvBOmHkFBX1FhTjfa/fTk7B9cscqjGMwZzaai6neObMBDdc0q3zBSYdYR9Br3tdCO8tB1dsM5rPsjmlcwZ1RMN9UlZZpjWYpBSGYqm3quodK668szPENbsqq80vRNDn5kNv2lXz45SjNeidp1x6cmSWiWiqqpLS9Yrf4+bBP3iz9gyKsKe7iaDXTTyd1Z5BHdFwYaKefJgoUXbfqXi6aqloS83yva8vL0q3lmgNeudVEz192pDC0PmC+ezqCmvp6iJ4rUDEQgAADiVJREFU3C4u29ICaCmKeqKsMRCRu0VkRESOFtj2MRFRItJl/v1fReQF8+eoiGRFpMPc1isiL5vbDtseo0NEHhGRk+bv0trONRLyeWj2exhx5BlUnwDb2BrgW797Pb/15t1V3X+1aAt584lzMPIFW9uDbNP5Ak0FHDCH3egwUf3gxDO4B7hl4aKIbANuBs5ba0qpv1FKHVRKHQT+BPiRUmrCdrcbze2HbGufAB5VSu0FHjX/Xla6W/yOPINIvHLFUjtXbW+vuON4tbEnkHM5xU/PTugQkaZirOYzHSaqH8qeqZRSTwATBTZ9Dvg4UKxg/3bgPgfHcCtwr3n7XuDdDu5TEz3NfoeeQeWzDOodK0yUyymOD00zFUvr/gJNxdy0r4cPvnHnkuTLNCtDVZetInIrMKCUerHI9hCGN/FN27ICHhaR50TkI7b1DUqpQfP2ELChxPN+REQOi8jh0dHRag7deMKWgOPS0uUYL7mWaQ16ySmYSWZ0f4Gmapr8Hj71rktrLqnWrBwVB/TME/2fYoSIivEu4MkFIaI3KaUGRKQHeEREXjW9jjxKKSUiRVuDlVJ3AXcBHDp0qHwLcRF6mv0MTydQShVN7mZzxkD7RmuasUtSPHNmnB2dITa3aZlmjWa9U41ncBGwC3hRRHqBrcDzIrLRts/7WRAiUkoNmL9HgG8D15ibhkVkE4D5e6SKY6qInuYAyUyO6USm6D7TNSiW1jOWMZiIpnS+QKNpICo2Bkqpl5VSPUqpnUqpnUA/cJVSaghARFqBtwLfte4jImERabZuY3gVVnXSA8Cd5u077fdbLqzy0tESSeSpBjUGVljs6TPjzCQyOl+g0TQITkpL7wOeBi4RkX4R+XCZu9wGPKyUitrWNgA/EZEXgWeB/6uU+r657TPA20XkJPBz5t/LSr4LuUQS2Wq8qkaXqJ6xPIPvHx0CdL5Ao2kUyuYMlFK3l9m+c8Hf92CUo9rXzgAHitx/HHhbueNYSizPYNiBZ1CNLlE9YxmDF/qm2N0VZkOL7rLVaBqB+iqCXyKsE1wpzyCfM2iwBLI9LHadDhFpNA1DQxqDJr+HkM9dsrx0bpZBY4WJAl43Po/xtdDJY42mcWhIYwBz5aXFsIxBS6Dx2uktb0jnCzSaxqHxznQmPc2lG8+m4imaAx48dSYnsRS0Br20Br10mwqvGo1m/dO4xqDFz7EL00W3R2K16RLVM79740WE9WxfjaahaNj/+J7mAI9NF+9vm4qnG66s1OK2K2sfxqPRaOqLxouBmPS0+ImlsswmC3chG7pEjekZaDSaxqNhjUF+FnKRJPJUPN1wukQajaZxaVhjMDcLuXASuZFzBhqNpvFoYGNgzUJe7BkopRo6Z6DRaBqPxjUGZhfyaAHPYDaZIZtT2jPQaDQNQ8Mag5aAB7/HVTBMZDWc6ZyBRqNpFBrWGIgIPS2Fu5Aj8caUotBoNI1LwxoDMLuQC4jVzekSac9Ao9E0Bg1tDDa0+BkpIGM9FbdmGWhjoNFoGoOGNgblPINGm2Wg0Wgal4Y2Bt3NfmaSGeKp7Lx1K2egE8gajaZRaGhjkB9ysyBUNBVLEfK58Xvcq3FYGo1Gs+I0tDGYazybHyqaiqV1vkCj0TQUjoyBiNwtIiMicrTAto+JiBKRLvPvG0QkIiIvmD+ftO17i4icEJFTIvIJ2/ouEfmpuf41EVmRmk5rFvIizyCeplWXlWo0mgbCqWdwD3DLwkUR2QbcDJxfsOnHSqmD5s9fmPu6gS8A7wD2A7eLyH5z/88Cn1NK7QEmgQ9X+kKqYUNz4VnIEe0ZaDSaBsORMVBKPQFMFNj0OeDjgHLwMNcAp5RSZ5RSKeCrwK0iIsBNwDfM/e4F3u3kuGqlLeTF517chTwV1/LVGo2msag6ZyAitwIDSqkXC2x+g4i8KCLfE5FLzbUtQJ9tn35zrROYUkplFqwXes6PiMhhETk8Ojpa7aHbH4/uZv8iGesprViq0WgajKqMgYiEgD8FPllg8/PADqXUAeAfge9Uf3jzUUrdpZQ6pJQ61N3dvSSP2d3sn+cZWIqlrVqxVKPRNBDVegYXAbuAF0WkF9gKPC8iG5VS00qpWQCl1IOA10wuDwDbbI+x1VwbB9pExLNgfUVY2IWcSOdIZXLaM9BoNA1FVcZAKfWyUqpHKbVTKbUTI7RzlVJqSEQ2mnkAROQa8znGgZ8Be83KIR/wfuABpZQCHgfeYz78ncB3a3pVFdDTHJhXWqqlKDQaTSPitLT0PuBp4BIR6ReRUtU+7wGOisiLwD8A71cGGeCjwEPAceB+pdQx8z5/DPyRiJzCyCF8qbqXUzk9zX4i8TSJtNGFrEXqNBpNI+IpvwsopW4vs32n7fbngc8X2e9B4MEC62cwqo1WnA22ITfbOkK2WQY6Z6DRaBqHhu5ABuhe0HgWscJE2jPQaDQNRMMbA0uSwmo802EijUbTiDS8MZgTqzONgTXlTIeJNBpNA9HwxqAj5MPjkvz4y6lYGp/HRcDb8G+NRqNpIBr+jOdyCV1Nc41nkXiKtqAXszpWo9FoGoKGNwZgqJfmw0RaikKj0TQg2hhgjb+cCxPpfIFGo2k0tDFggWcQT+vZxxqNpuHQxgCjvHQimiKVyRGJpbQUhUajaTi0MWCuvHRsNslUXOcMNBpN46GNAXONZ/2TcWKpLG165KVGo2kwtDHASCADvDY8A0CrDhNpNJoGQxsDjJkGACdNY6DDRBqNptHQxgDobPLjEjhhGQNdWqrRaBoMbQwAt0vobPJzcngW0J6BRqNpPLQxMOlp9jMeNeSrdc5Ao9E0GtoYmFjlpaA9A41G03hoY2BilZe6XUKT39EAOI1Go1k3lDUGInK3iIyIyNEC2z4mIkpEusy/7xCRl0TkZRF5SkQO2PbtNddfEJHDtvUOEXlERE6av9uX6sVVgmUMtGKpRqNpRJx4BvcAtyxcFJFtwM3AedvyWeCtSqnLgf8O3LXgbjcqpQ4qpQ7Z1j4BPKqU2gs8av694vSYYSKtS6TRaBqRssZAKfUEMFFg0+eAjwPKtu9TSqlJ889ngK0OjuFW4F7z9r3Aux3cZ8mxewYajUbTaFSVMxCRW4EBpdSLJXb7MPA9298KeFhEnhORj9jWNyilBs3bQ8CGEs/7ERE5LCKHR0dHqzn0oliegZai0Gg0jUjFmVIRCQF/ihEiKrbPjRjG4E225TcppQZEpAd4REReNb2OPEopJSKKIiil7sIMPR06dKjoftWgPQONRtPIVOMZXATsAl4UkV6MUNDzIrIRQESuAP43cKtSaty6k1JqwPw9AnwbuMbcNCwim8z7bgJGqnsptdHd7EcEWrQx0Gg0DUjFxkAp9bJSqkcptVMptRPoB65SSg2JyHbgW8CvK6Ves+4jImERabZuY3gVVnXSA8Cd5u07ge9W/WpqwOt28d/e+Trec7WTNIdGo9GsL8qGiUTkPuAGoEtE+oFPKaW+VGT3TwKdwD+Z5ZkZs3JoA/Btc80D/JtS6vvmfT4D3C8iHwbOAe+t/uXUxn968+7VemqNRqNZVUSpJQ29rxiHDh1Shw8fLr+jRqPRaPKIyHMLyvsB3YGs0Wg0GrQx0Gg0Gg3aGGg0Go0GbQw0Go1GgzYGGo1Go0EbA41Go9GgjYFGo9FoqOM+AxEZxWhSq4YuYGwJD6de0e/DHPq9MNDvg8F6fh92KKW6Fy7WrTGoBRE5XKjpotHQ78Mc+r0w0O+DQSO+DzpMpNFoNBptDDQajUbTuMZg4TjORkW/D3Po98JAvw8GDfc+NGTOQKPRaDTzaVTPQKPRaDQ2tDHQaDQaTeMZAxG5RUROiMgpEfnEah/PSiEid4vIiIgcta11iMgjInLS/N2+mse4EojINhF5XEReEZFjIvIH5npDvRciEhCRZ0XkRfN9+HNzfZeI/NT8//iaiPhW+1hXAhFxi8gREfkP8++Gex8ayhiIiBv4AvAOYD9wu4jsX92jWjHuAW5ZsPYJ4FGl1F7gUfPv9U4G+JhSaj9wHfB75neg0d6LJHCTUuoAcBC4RUSuAz4LfE4ptQeYBD68ise4kvwBcNz2d8O9Dw1lDIBrgFNKqTNKqRTwVeDWVT6mFUEp9QQwsWD5VuBe8/a9wLtX9KBWAfX/t3f3rlEEcRjHvw9JREEkKCIhpwRBsBItRYsg2MRgJSIo5F+wEEEbQbCV2KudCEElphWSwkrEF7CwslI5kyqojaA+FjPg4Ut7CzfPBw5mZrcYfrD7m52Z3bP7tl/W8hfKDWCaxmLh4mutTtSfgRPAg9o+8nEAkNQDTgG3a100GIfWksE08H6g/qG2tWqP7X4tf6L8V3UzJM0AR4BnNBiLOjXyGtgAngDvgE3b3+sprVwfi8Bl4Get76LBOLSWDOI/XPYYN7PPWNJ24CFw0fbnwWOtxML2D9uHgR7lqflgx10aOknzwIbtF133pWvjXXdgyD4CewfqvdrWqnVJU7b7kqYoI8SRJ2mCkgju2X5Um5uMBYDtTUlrwFFgUtJ4HRW3cH0cA05LmgO2AjuAW7QXh+aeDJ4DB+pOgS3AOWCl4z51aQVYqOUF4HGHfRmKOh98B3hr++bAoaZiIWm3pMla3gacpKyfrAFn6mkjHwfbV2z3bM9Q7gerts/TWBygwTeQ6whgERgD7tq+0XGXhkLSfWCW8mnedeAasAwsAfsonwM/a/vPReaRIuk48BR4w+854quUdYNmYiHpEGVhdIwyKFyyfV3SfsrGip3AK+CC7W/d9XR4JM0Cl2zPtxiH5pJBRET8rbVpooiI+Ickg4iISDKIiIgkg4iIIMkgIiJIMoiICJIMIiIC+AX0Hpt6c572DAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = np.concatenate((np.load('episode_20-loss.npy'),np.load('episode_25 DeepQ_notExact-loss.npy')))\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "dNuC29iCuOqn",
        "outputId": "06d40486-b3f0-467a-980c-36f6ad4dc3b9"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improvments "
      ],
      "metadata": {
        "id": "AFPwq82_zhxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Improvments that could be implemented : \n",
        "\n",
        "# In the heuristic, weigth the probability that each car take a request between t and t + delta_t, and use that to have more accurate estimation\n",
        "# Ranking of requests in the repositionnement\n",
        "# Categorize requests for DistilQ into 4 category : (long and fast request) / (short and slow request) / (long and slow request) / (short and fast request) to make learning easier.\n",
        "# Better deal with the overlap issue when assigning we want to assign 2 requests to a car!\n",
        "# Do a pre-processing on the neighbours of every punkt of the grid to have better results in the heuristic (a long task) \n",
        "# We supose actually in the heuristic that cars in 104 don't exist between t and t + delta_t if there tasks aren't done but they can take requests! So that would be a good correction!\n",
        "# Try : Discount_rate = 1.\n",
        "# Make the Advantage work."
      ],
      "metadata": {
        "id": "FzWZYsNRAQwH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bBlNAPn87XKo",
        "FRulRdiMCUmf",
        "ql8q6KA7vHkU",
        "hbSmoP09Jofr",
        "SOdlaLeetAz9",
        "D_cPuEDQueaY",
        "EKKBZxTVuvAn",
        "ssFTnV6-L1iX",
        "V-zJSc1l3_sJ",
        "SaGVNnBZHe9l",
        "tmwgL2tg4euv",
        "vVYnm0T8jGxL",
        "G4GcdUcFxF4l",
        "AFPwq82_zhxQ"
      ],
      "name": "Ride Hailing_Sing_Duvshani.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}